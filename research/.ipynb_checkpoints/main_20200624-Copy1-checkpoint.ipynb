{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import os\n",
    "import sys\n",
    "from scipy import signal\n",
    "import skinematics as skin\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from scipy.stats import norm\n",
    "#from scipy import signal\n",
    "from taskcaller import taskcaller\n",
    "from taskcaller_train1 import taskcaller_train1\n",
    "import random\n",
    "# modularized library import\n",
    "from train_test_split_k import train_test_split_k\n",
    "from rms import rms    \n",
    "from copy import copy\n",
    "from datetime import datetime\n",
    "from maml import *\n",
    "\n",
    "from math import pi\n",
    "from math import cos\n",
    "from math import floor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version:  3.6.10 |Anaconda, Inc.| (default, May  7 2020, 19:46:08) [MSC v.1916 64 bit (AMD64)]\n",
      "Tensorflow version:  2.2.0\n"
     ]
    }
   ],
   "source": [
    "dtype=\"float64\"\n",
    "tf.keras.backend.set_floatx(dtype)\n",
    "print('Python version: ', sys.version)\n",
    "print('Tensorflow version: ', tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../dataset/trainingtask1.csv loaded...\n",
      "\n",
      "Anticipation time: 300ms\n",
      "\n",
      "../dataset/trainingtask2.csv loaded...\n",
      "\n",
      "Anticipation time: 300ms\n",
      "\n",
      "../dataset/trainingtask3.csv loaded...\n",
      "\n",
      "Anticipation time: 300ms\n",
      "\n",
      "../dataset/trainingtask4.csv loaded...\n",
      "\n",
      "Anticipation time: 300ms\n",
      "\n",
      "../dataset/trainingtask5.csv loaded...\n",
      "\n",
      "Anticipation time: 300ms\n",
      "\n",
      "../dataset/trainingtask6.csv loaded...\n",
      "\n",
      "Anticipation time: 300ms\n",
      "\n",
      "../dataset/trainingtask7.csv loaded...\n",
      "\n",
      "Anticipation time: 300ms\n",
      "\n",
      "../dataset/trainingtask8.csv loaded...\n",
      "\n",
      "Anticipation time: 300ms\n",
      "\n",
      "../dataset/trainingtask9.csv loaded...\n",
      "\n",
      "Anticipation time: 300ms\n",
      "\n",
      "../dataset/trainingtask9.csv loaded...\n",
      "\n",
      "Anticipation time: 300ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "S=30\n",
    "# np.random.seed(S)\n",
    "# random.seed(S)\n",
    "# tf.random.set_seed(S)\n",
    "\n",
    "system_rate = 60\n",
    "k_train = 100\n",
    "x_train1, t_train1, x_val1, t_val1,input_nm, target_nm, data_length, DELAY_SIZE, train_eule_data, anticipation_size, train_time_data = taskcaller_train1('../dataset/trainingtask1.csv', system_rate, k_train)\n",
    "x_train2, t_train2, x_val2, t_val2,_, _, _, _, _, _, _ = taskcaller_train1('../dataset/trainingtask2.csv', system_rate, k_train)\n",
    "x_train3, t_train3, x_val3, t_val3,_, _, _, _, _, _, _ = taskcaller_train1('../dataset/trainingtask3.csv', system_rate, k_train)\n",
    "x_train4, t_train4, x_val4, t_val4, _, _, _, _, _, _, _ = taskcaller_train1('../dataset/trainingtask4.csv', system_rate, k_train)\n",
    "x_train5, t_train5, x_val5, t_val5,_, _, _, _, _, _, _ = taskcaller_train1('../dataset/trainingtask5.csv', system_rate, k_train)\n",
    "x_train6, t_train6, x_val6, t_val6,_, _, _, _, _, _, _ = taskcaller_train1('../dataset/trainingtask6.csv', system_rate, k_train)\n",
    "x_train7, t_train7, x_val7, t_val7,_, _, _, _, _, _, _ = taskcaller_train1('../dataset/trainingtask7.csv', system_rate, k_train)\n",
    "x_train8, t_train8, x_val8, t_val8,_, _, _, _, _, _, _ = taskcaller_train1('../dataset/trainingtask8.csv', system_rate, k_train)\n",
    "x_train9, t_train9, x_val9, t_val9,_, _, _, _, _, _, _ = taskcaller_train1('../dataset/trainingtask9.csv', system_rate, k_train)\n",
    "x_train10, t_train10, x_val10, t_val10,_, _, _, _, _, _, _ = taskcaller_train1('../dataset/trainingtask9.csv', system_rate, k_train)\n",
    "#x_seq10, t_seq10, _, _,_, _, _, _, _, _, _ = taskcaller('trainingtask10.csv', system_rate, k)\n",
    "\n",
    "\n",
    "traintaskx = [x_train1 , x_train2 , x_train3 , x_train4,x_train5,x_train6,x_train7,x_train8,x_train9,x_train10]\n",
    "traintaskt = [t_train1 , t_train2 , t_train3 , t_train4,t_train5,t_train6,t_train7, t_train8, t_train9,t_train10]\n",
    "\n",
    "valtaskx = [x_val1,x_val2,x_val3,x_val4,x_val5,x_val6,x_val7,x_val8,x_val9,x_val10]\n",
    "valtaskt = [t_val1,t_val2,t_val3,t_val4,t_val5,t_val6,t_val7,t_val8,t_val9,t_val10]\n",
    "\n",
    "numberoftask = len(traintaskx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftml1 = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.InputLayer(input_shape=(DELAY_SIZE, input_nm)),\n",
    "        tf.keras.layers.Conv1D(27, DELAY_SIZE, activation=tf.nn.relu, input_shape=(DELAY_SIZE, input_nm), use_bias=True, \n",
    "        kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(9, activation=tf.nn.relu, use_bias=True, kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(target_nm, activation='linear', use_bias=True, kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftml2 = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.InputLayer(input_shape=(DELAY_SIZE, input_nm)),\n",
    "        tf.keras.layers.Conv1D(27, DELAY_SIZE, activation=tf.nn.relu, input_shape=(DELAY_SIZE, input_nm), use_bias=True, \n",
    "        kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(9, activation=tf.nn.relu, use_bias=True, kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(target_nm, activation='linear', use_bias=True, kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftml3 = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.InputLayer(input_shape=(DELAY_SIZE, input_nm)),\n",
    "        tf.keras.layers.Conv1D(27, DELAY_SIZE, activation=tf.nn.relu, input_shape=(DELAY_SIZE, input_nm), use_bias=True, \n",
    "        kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(9, activation=tf.nn.relu, use_bias=True, kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(target_nm, activation='linear', use_bias=True, kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftml4 = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.InputLayer(input_shape=(DELAY_SIZE, input_nm)),\n",
    "        tf.keras.layers.Conv1D(27, DELAY_SIZE, activation=tf.nn.relu, input_shape=(DELAY_SIZE, input_nm), use_bias=True, \n",
    "        kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(9, activation=tf.nn.relu, use_bias=True, kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(target_nm, activation='linear', use_bias=True, kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftml5 = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.InputLayer(input_shape=(DELAY_SIZE, input_nm)),\n",
    "        tf.keras.layers.Conv1D(27, DELAY_SIZE, activation=tf.nn.relu, input_shape=(DELAY_SIZE, input_nm), use_bias=True, \n",
    "        kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(9, activation=tf.nn.relu, use_bias=True, kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(target_nm, activation='linear', use_bias=True, kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftml6 = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.InputLayer(input_shape=(DELAY_SIZE, input_nm)),\n",
    "        tf.keras.layers.Conv1D(27, DELAY_SIZE, activation=tf.nn.relu, input_shape=(DELAY_SIZE, input_nm), use_bias=True, \n",
    "        kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(9, activation=tf.nn.relu, use_bias=True, kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(target_nm, activation='linear', use_bias=True, kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_procedure(model,dtx, dty, lr = 0.001, grad_step =10, ca=True):\n",
    "    \n",
    "    all_loss = []\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate = lr)\n",
    "    lr_min = lr/20\n",
    "    for step in range (grad_step):\n",
    "        total_loss = 0\n",
    "        if ca:\n",
    "            lr_ca = cosine_annealing(step, grad_step, lr, lr_min)\n",
    "            optimizer = tf.keras.optimizers.Adam(learning_rate = lr_ca)\n",
    "        for i in range(len(dtx)):\n",
    "            with tf.GradientTape() as update:\n",
    "                _,loss = model_func(model, dtx[i], dty[i])\n",
    "            gradient = update.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradient, model.trainable_variables))\n",
    "            total_loss+=loss\n",
    "        all_loss.append(total_loss/len(dtx))\n",
    "        print('Step{} : loss = {}'.format(step,total_loss/len(dtx)))\n",
    "    return model, all_loss  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task  0\n",
      "Meta Update\n",
      "Training is starting\n",
      "Step 0 : loss = 6.618682384490967\n",
      "Step 1 : loss = 6.618029594421387\n",
      "Step 2 : loss = 6.617382049560547\n",
      "Step 3 : loss = 6.616733551025391\n",
      "Step 4 : loss = 6.616084098815918\n",
      "Step 5 : loss = 6.615423202514648\n",
      "Step 6 : loss = 6.6147541999816895\n",
      "Step 7 : loss = 6.614074230194092\n",
      "Step 8 : loss = 6.613388538360596\n",
      "Step 9 : loss = 6.612706184387207\n",
      "Update Procedure\n",
      "Step0 : loss = 6.612705707550049\n",
      "Step1 : loss = 6.598124980926514\n",
      "Step2 : loss = 6.5823893547058105\n",
      "Step3 : loss = 6.5668416023254395\n",
      "Step4 : loss = 6.552420616149902\n",
      "Step5 : loss = 6.539622783660889\n",
      "Step6 : loss = 6.529259204864502\n",
      "Step7 : loss = 6.521468639373779\n",
      "Step8 : loss = 6.516040802001953\n",
      "Step9 : loss = 6.512491226196289\n",
      "Data stream Batch- 0 : loss = 4.7619712352752686\n",
      "Meta Update\n",
      "Training is starting\n",
      "Step 0 : loss = 6.9408934116363525\n",
      "Step 1 : loss = 6.939679145812988\n",
      "Step 2 : loss = 6.938460350036621\n",
      "Step 3 : loss = 6.937238454818726\n",
      "Step 4 : loss = 6.93601393699646\n",
      "Step 5 : loss = 6.934786081314087\n",
      "Step 6 : loss = 6.933550834655762\n",
      "Step 7 : loss = 6.932312965393066\n",
      "Step 8 : loss = 6.931070804595947\n",
      "Step 9 : loss = 6.929825067520142\n",
      "Update Procedure\n",
      "Step0 : loss = 6.921811819076538\n",
      "Step1 : loss = 6.8872270584106445\n",
      "Step2 : loss = 6.854473829269409\n",
      "Step3 : loss = 6.823986768722534\n",
      "Step4 : loss = 6.797569751739502\n",
      "Step5 : loss = 6.774646759033203\n",
      "Step6 : loss = 6.755842924118042\n",
      "Step7 : loss = 6.741904973983765\n",
      "Step8 : loss = 6.732276916503906\n",
      "Step9 : loss = 6.72595739364624\n",
      "Data stream Batch- 1 : loss = 4.720237731933594\n",
      "Meta Update\n",
      "Training is starting\n",
      "Step 0 : loss = 7.141917069753011\n",
      "Step 1 : loss = 7.141523996988933\n",
      "Step 2 : loss = 7.141129732131958\n",
      "Step 3 : loss = 7.140737136205037\n",
      "Step 4 : loss = 7.140345970789592\n",
      "Step 5 : loss = 7.139954010645549\n",
      "Step 6 : loss = 7.139562129974365\n",
      "Step 7 : loss = 7.139169692993164\n",
      "Step 8 : loss = 7.138779004414876\n",
      "Step 9 : loss = 7.138386408487956\n",
      "Update Procedure\n",
      "Step0 : loss = 9.369975725809732\n",
      "Step1 : loss = 9.330853780110678\n",
      "Step2 : loss = 9.292831103006998\n",
      "Step3 : loss = 9.256741046905518\n",
      "Step4 : loss = 9.224586645762125\n",
      "Step5 : loss = 9.196881612141928\n",
      "Step6 : loss = 9.174903710683187\n",
      "Step7 : loss = 9.158190568288168\n",
      "Step8 : loss = 9.146285057067871\n",
      "Step9 : loss = 9.138497988382975\n",
      "Data stream Batch- 2 : loss = 4.641624212265015\n",
      "Meta Update\n",
      "Training is starting\n",
      "Step 0 : loss = 3.6435263355573015\n",
      "Step 1 : loss = 3.6432057817776995\n",
      "Step 2 : loss = 3.6428862015406294\n",
      "Step 3 : loss = 3.6425667007764178\n",
      "Step 4 : loss = 3.6422475775082903\n",
      "Step 5 : loss = 3.641930401325226\n",
      "Step 6 : loss = 3.6416122515996294\n",
      "Step 7 : loss = 3.641294042269389\n",
      "Step 8 : loss = 3.640975773334503\n",
      "Step 9 : loss = 3.640657862027486\n",
      "Update Procedure\n",
      "Step0 : loss = 8.732234358787537\n",
      "Step1 : loss = 8.680952429771423\n",
      "Step2 : loss = 8.635953664779663\n",
      "Step3 : loss = 8.58789324760437\n",
      "Step4 : loss = 8.549096584320068\n",
      "Step5 : loss = 8.511418581008911\n",
      "Step6 : loss = 8.482924580574036\n",
      "Step7 : loss = 8.460956573486328\n",
      "Step8 : loss = 8.445390939712524\n",
      "Step9 : loss = 8.435302734375\n",
      "Data stream Batch- 3 : loss = 4.519147038459778\n",
      "Meta Update\n",
      "Training is starting\n",
      "Step 0 : loss = 2.896467467149099\n",
      "Step 1 : loss = 2.894791122277578\n",
      "Step 2 : loss = 2.8931209127108253\n",
      "Step 3 : loss = 2.891463613510132\n",
      "Step 4 : loss = 2.889815032482147\n",
      "Step 5 : loss = 2.8881663401921593\n",
      "Step 6 : loss = 2.886528583367666\n",
      "Step 7 : loss = 2.8848968942960105\n",
      "Step 8 : loss = 2.883273037274679\n",
      "Step 9 : loss = 2.8816554029782617\n",
      "Update Procedure\n",
      "Step0 : loss = 8.936216354370117\n",
      "Step1 : loss = 8.887905693054199\n",
      "Step2 : loss = 8.836411666870116\n",
      "Step3 : loss = 8.784070587158203\n",
      "Step4 : loss = 8.734506320953368\n",
      "Step5 : loss = 8.690971565246581\n",
      "Step6 : loss = 8.654546165466309\n",
      "Step7 : loss = 8.626985168457031\n",
      "Step8 : loss = 8.606927871704102\n",
      "Step9 : loss = 8.593623161315918\n",
      "Data stream Batch- 4 : loss = 4.337743163108826\n",
      "Meta Update\n",
      "Training is starting\n",
      "Step 0 : loss = 1.9230669452084435\n",
      "Step 1 : loss = 1.922891021437115\n",
      "Step 2 : loss = 1.922715331448449\n",
      "Step 3 : loss = 1.9225399043824938\n",
      "Step 4 : loss = 1.9223647144105698\n",
      "Step 5 : loss = 1.9221897131866879\n",
      "Step 6 : loss = 1.9220148980617522\n",
      "Step 7 : loss = 1.9218406902419194\n",
      "Step 8 : loss = 1.921666309568617\n",
      "Step 9 : loss = 1.921492185195287\n",
      "Update Procedure\n",
      "Step0 : loss = 8.597716887791952\n",
      "Step1 : loss = 8.511770566304525\n",
      "Step2 : loss = 8.412499586741129\n",
      "Step3 : loss = 8.310543219248453\n",
      "Step4 : loss = 8.211788256963095\n",
      "Step5 : loss = 8.123355309168497\n",
      "Step6 : loss = 8.05004366238912\n",
      "Step7 : loss = 7.993801196416219\n",
      "Step8 : loss = 7.953897992769877\n",
      "Step9 : loss = 7.927414377530416\n",
      "Data stream Batch- 5 : loss = 4.019535422325134\n",
      "Meta Update\n",
      "Training is starting\n",
      "Step 0 : loss = 1.05570423281382\n",
      "Step 1 : loss = 1.0556038933613945\n",
      "Step 2 : loss = 1.0555034458637238\n",
      "Step 3 : loss = 1.0554028915980507\n",
      "Step 4 : loss = 1.0553029602955257\n",
      "Step 5 : loss = 1.0552027854181472\n",
      "Step 6 : loss = 1.0551027135716544\n",
      "Step 7 : loss = 1.0550026521796272\n",
      "Step 8 : loss = 1.0549026632119738\n",
      "Step 9 : loss = 1.0548026216881616\n",
      "Update Procedure\n",
      "Step0 : loss = 7.560071229934692\n",
      "Step1 : loss = 7.411860942840576\n",
      "Step2 : loss = 7.26191817011152\n",
      "Step3 : loss = 7.114214250019619\n",
      "Step4 : loss = 6.978663955415998\n",
      "Step5 : loss = 6.860079833439419\n",
      "Step6 : loss = 6.762115274156843\n",
      "Step7 : loss = 6.68856178011213\n",
      "Step8 : loss = 6.63709248815264\n",
      "Step9 : loss = 6.603438547679356\n",
      "Data stream Batch- 6 : loss = 3.471100687980652\n",
      "Meta Update\n",
      "Training is starting\n",
      "Step 0 : loss = 0.5462391472998119\n",
      "Step 1 : loss = 0.5461508616982471\n",
      "Step 2 : loss = 0.5460629177353685\n",
      "Step 3 : loss = 0.5459749708336497\n",
      "Step 4 : loss = 0.5458870333634198\n",
      "Step 5 : loss = 0.5457992184552408\n",
      "Step 6 : loss = 0.5457113847727814\n",
      "Step 7 : loss = 0.545623568143873\n",
      "Step 8 : loss = 0.5455358657690268\n",
      "Step 9 : loss = 0.5454481543943522\n",
      "Update Procedure\n",
      "Step0 : loss = 6.158601611852646\n",
      "Step1 : loss = 5.9711670726537704\n",
      "Step2 : loss = 5.802279576659203\n",
      "Step3 : loss = 5.6716543436050415\n",
      "Step4 : loss = 5.567662820219994\n",
      "Step5 : loss = 5.482139952480793\n",
      "Step6 : loss = 5.414252556860447\n",
      "Step7 : loss = 5.365604370832443\n",
      "Step8 : loss = 5.331823945045471\n",
      "Step9 : loss = 5.310449793934822\n",
      "Data stream Batch- 7 : loss = 2.8843499422073364\n",
      "Task  1\n",
      "Meta Update\n",
      "Training is starting\n",
      "Step 0 : loss = 1.9920138319333394\n",
      "Step 1 : loss = 1.9911643465360005\n",
      "Step 2 : loss = 1.9903161525726318\n",
      "Step 3 : loss = 1.9894930322964985\n",
      "Step 4 : loss = 1.9886468052864075\n",
      "Step 5 : loss = 1.9878016710281372\n",
      "Step 6 : loss = 1.9869573314984639\n",
      "Step 7 : loss = 1.9861138264338176\n",
      "Step 8 : loss = 1.9852715531984966\n",
      "Step 9 : loss = 1.9844303727149963\n",
      "Update Procedure\n",
      "Step0 : loss = 4.307424545288086\n",
      "Step1 : loss = 4.233071327209473\n",
      "Step2 : loss = 4.1607890129089355\n",
      "Step3 : loss = 4.093539237976074\n",
      "Step4 : loss = 4.033942699432373\n",
      "Step5 : loss = 3.9837353229522705\n",
      "Step6 : loss = 3.9437437057495117\n",
      "Step7 : loss = 3.9138824939727783\n",
      "Step8 : loss = 3.8931796550750732\n",
      "Step9 : loss = 3.8797149658203125\n",
      "Data stream Batch- 0 : loss = 13.084500789642334\n",
      "Meta Update\n",
      "Training is starting\n",
      "Step 0 : loss = 1.940136194229126\n",
      "Step 1 : loss = 1.9393621285756428\n",
      "Step 2 : loss = 1.9385886987050374\n",
      "Step 3 : loss = 1.937816043694814\n",
      "Step 4 : loss = 1.9370448191960652\n",
      "Step 5 : loss = 1.9362997810045879\n",
      "Step 6 : loss = 1.935531775156657\n",
      "Step 7 : loss = 1.934764325618744\n",
      "Step 8 : loss = 1.933996359507243\n",
      "Step 9 : loss = 1.933269460995992\n",
      "Update Procedure\n",
      "Step0 : loss = 4.136431932449341\n",
      "Step1 : loss = 4.017350673675537\n",
      "Step2 : loss = 3.90849769115448\n",
      "Step3 : loss = 3.8118287324905396\n",
      "Step4 : loss = 3.727887511253357\n",
      "Step5 : loss = 3.6566145420074463\n",
      "Step6 : loss = 3.5996721982955933\n",
      "Step7 : loss = 3.5576467514038086\n",
      "Step8 : loss = 3.5291781425476074\n",
      "Step9 : loss = 3.510712146759033\n",
      "Data stream Batch- 1 : loss = 13.063218116760254\n",
      "Meta Update\n",
      "Training is starting\n",
      "Step 0 : loss = 1.8583667079607646\n",
      "Step 1 : loss = 1.857500155766805\n",
      "Step 2 : loss = 1.856634259223938\n",
      "Step 3 : loss = 1.855767289797465\n",
      "Step 4 : loss = 1.854898452758789\n",
      "Step 5 : loss = 1.854029933611552\n",
      "Step 6 : loss = 1.853156824906667\n",
      "Step 7 : loss = 1.852280815442403\n",
      "Step 8 : loss = 1.8514028986295064\n",
      "Step 9 : loss = 1.8505210876464844\n",
      "Update Procedure\n",
      "Step0 : loss = 4.144876162211101\n",
      "Step1 : loss = 3.966486612955729\n",
      "Step2 : loss = 3.8029189904530845\n",
      "Step3 : loss = 3.6561025778452554\n",
      "Step4 : loss = 3.530604283014933\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step5 : loss = 3.4322702884674072\n",
      "Step6 : loss = 3.35878849029541\n",
      "Step7 : loss = 3.3052923679351807\n",
      "Step8 : loss = 3.269319772720337\n",
      "Step9 : loss = 3.24605663617452\n",
      "Data stream Batch- 2 : loss = 13.038037300109863\n",
      "Meta Update\n",
      "Training is starting\n",
      "Step 0 : loss = 1.7437420288721721\n",
      "Step 1 : loss = 1.7430972655614216\n",
      "Step 2 : loss = 1.7424536347389221\n",
      "Step 3 : loss = 1.7418298721313477\n",
      "Step 4 : loss = 1.7411954998970032\n",
      "Step 5 : loss = 1.7405612270037334\n",
      "Step 6 : loss = 1.7399268349011738\n",
      "Step 7 : loss = 1.739292581876119\n",
      "Step 8 : loss = 1.7386581500371296\n",
      "Step 9 : loss = 1.73806365331014\n",
      "Update Procedure\n",
      "Step0 : loss = 2.8060190677642822\n",
      "Step1 : loss = 2.6344708800315857\n",
      "Step2 : loss = 2.5225924849510193\n",
      "Step3 : loss = 2.459694564342499\n",
      "Step4 : loss = 2.4237018525600433\n",
      "Step5 : loss = 2.398660361766815\n",
      "Step6 : loss = 2.379790037870407\n",
      "Step7 : loss = 2.366167366504669\n",
      "Step8 : loss = 2.3573683202266693\n",
      "Step9 : loss = 2.3520275950431824\n",
      "Data stream Batch- 3 : loss = 13.016119003295898\n",
      "Meta Update\n",
      "Training is starting\n",
      "Step 0 : loss = 1.6535661220550537\n",
      "Step 1 : loss = 1.6530454357465107\n",
      "Step 2 : loss = 1.6525242726008098\n",
      "Step 3 : loss = 1.652002473672231\n",
      "Step 4 : loss = 1.6514792243639629\n",
      "Step 5 : loss = 1.6509559154510498\n",
      "Step 6 : loss = 1.6504323085149128\n",
      "Step 7 : loss = 1.6499077280362446\n",
      "Step 8 : loss = 1.649382213751475\n",
      "Step 9 : loss = 1.6488553682963054\n",
      "Update Procedure\n",
      "Step0 : loss = 2.84044234752655\n",
      "Step1 : loss = 2.815561580657959\n",
      "Step2 : loss = 2.797492027282715\n",
      "Step3 : loss = 2.779957675933838\n",
      "Step4 : loss = 2.7659849643707277\n",
      "Step5 : loss = 2.753117060661316\n",
      "Step6 : loss = 2.7415409803390505\n",
      "Step7 : loss = 2.732771563529968\n",
      "Step8 : loss = 2.7264044523239135\n",
      "Step9 : loss = 2.722449851036072\n",
      "Data stream Batch- 4 : loss = 13.005481243133545\n",
      "Meta Update\n",
      "Training is starting\n",
      "Step 0 : loss = 1.5953855514526367\n",
      "Step 1 : loss = 1.5947813193003337\n",
      "Step 2 : loss = 1.5941763520240784\n",
      "Step 3 : loss = 1.5935705304145813\n",
      "Step 4 : loss = 1.5929639339447021\n",
      "Step 5 : loss = 1.592356026172638\n",
      "Step 6 : loss = 1.5917465289433796\n",
      "Step 7 : loss = 1.5911362767219543\n",
      "Step 8 : loss = 1.5905253489812214\n",
      "Step 9 : loss = 1.5899131298065186\n",
      "Update Procedure\n",
      "Step0 : loss = 4.747127374013265\n",
      "Step1 : loss = 4.728356719017029\n",
      "Step2 : loss = 4.712651908397675\n",
      "Step3 : loss = 4.698504368464152\n",
      "Step4 : loss = 4.684429367383321\n",
      "Step5 : loss = 4.671695411205292\n",
      "Step6 : loss = 4.660836140314738\n",
      "Step7 : loss = 4.652018070220947\n",
      "Step8 : loss = 4.64562447865804\n",
      "Step9 : loss = 4.641572733720143\n",
      "Data stream Batch- 5 : loss = 12.994407653808594\n",
      "Meta Update\n",
      "Training is starting\n",
      "Step 0 : loss = 1.5639132062594097\n",
      "Step 1 : loss = 1.5633007685343425\n",
      "Step 2 : loss = 1.5626914501190186\n",
      "Step 3 : loss = 1.562082866827647\n",
      "Step 4 : loss = 1.5614734093348186\n",
      "Step 5 : loss = 1.5608632167180378\n",
      "Step 6 : loss = 1.5602521498998005\n",
      "Step 7 : loss = 1.5596413016319275\n",
      "Step 8 : loss = 1.559034526348114\n",
      "Step 9 : loss = 1.5584161877632141\n",
      "Update Procedure\n",
      "Step0 : loss = 5.985680818557739\n",
      "Step1 : loss = 5.966142007282802\n",
      "Step2 : loss = 5.946414589881897\n",
      "Step3 : loss = 5.92573881149292\n",
      "Step4 : loss = 5.903345959527152\n",
      "Step5 : loss = 5.879661066191537\n",
      "Step6 : loss = 5.8604316881724765\n",
      "Step7 : loss = 5.845434410231454\n",
      "Step8 : loss = 5.834870270320347\n",
      "Step9 : loss = 5.827843291418893\n",
      "Data stream Batch- 6 : loss = 12.949671268463135\n",
      "Meta Update\n",
      "Training is starting\n",
      "Step 0 : loss = 1.542231261730194\n",
      "Step 1 : loss = 1.5416555007298787\n",
      "Step 2 : loss = 1.5410791436831157\n",
      "Step 3 : loss = 1.5405017733573914\n",
      "Step 4 : loss = 1.5399245222409566\n",
      "Step 5 : loss = 1.5393478671709697\n",
      "Step 6 : loss = 1.538770616054535\n",
      "Step 7 : loss = 1.538191338380178\n",
      "Step 8 : loss = 1.5376155376434326\n",
      "Step 9 : loss = 1.5370443860689798\n",
      "Update Procedure\n",
      "Step0 : loss = 6.792231187224388\n",
      "Step1 : loss = 6.760278478264809\n",
      "Step2 : loss = 6.731272026896477\n",
      "Step3 : loss = 6.70293627679348\n",
      "Step4 : loss = 6.676298201084137\n",
      "Step5 : loss = 6.652716115117073\n",
      "Step6 : loss = 6.632857263088226\n",
      "Step7 : loss = 6.618019953370094\n",
      "Step8 : loss = 6.60731628537178\n",
      "Step9 : loss = 6.600355491042137\n",
      "Data stream Batch- 7 : loss = 12.915499210357666\n",
      "Task  2\n",
      "Meta Update\n",
      "Training is starting\n",
      "Step 0 : loss = 1.9605125299640118\n",
      "Step 1 : loss = 1.9604948435244816\n",
      "Step 2 : loss = 1.9604773160454536\n",
      "Step 3 : loss = 1.9604595328282033\n",
      "Step 4 : loss = 1.960442002368943\n",
      "Step 5 : loss = 1.960424338502898\n",
      "Step 6 : loss = 1.9604068110150004\n",
      "Step 7 : loss = 1.9603890449843473\n",
      "Step 8 : loss = 1.9603715203584187\n",
      "Step 9 : loss = 1.9603539728633468\n",
      "Update Procedure\n",
      "Step0 : loss = 5.022233486175537\n",
      "Step1 : loss = 4.9377288818359375\n",
      "Step2 : loss = 4.858992099761963\n",
      "Step3 : loss = 4.791241645812988\n",
      "Step4 : loss = 4.737623691558838\n",
      "Step5 : loss = 4.696845531463623\n",
      "Step6 : loss = 4.665063858032227\n",
      "Step7 : loss = 4.641450881958008\n",
      "Step8 : loss = 4.6251540184021\n",
      "Step9 : loss = 4.614856719970703\n",
      "Data stream Batch- 0 : loss = 4.841331958770752\n",
      "Meta Update\n",
      "Training is starting\n",
      "Step 0 : loss = 1.9577356770812047\n",
      "Step 1 : loss = 1.9577178292390371\n",
      "Step 2 : loss = 1.9577002396570549\n",
      "Step 3 : loss = 1.9576825649581022\n",
      "Step 4 : loss = 1.9576645950003275\n",
      "Step 5 : loss = 1.9576467642264943\n",
      "Step 6 : loss = 1.9576290496580657\n",
      "Step 7 : loss = 1.9576110000175144\n",
      "Step 8 : loss = 1.9575933880867467\n",
      "Step 9 : loss = 1.957575815321789\n",
      "Update Procedure\n",
      "Step0 : loss = 9.886184453964233\n",
      "Step1 : loss = 9.786915302276611\n",
      "Step2 : loss = 9.701747417449951\n",
      "Step3 : loss = 9.6202712059021\n",
      "Step4 : loss = 9.550032377243042\n",
      "Step5 : loss = 9.495035886764526\n",
      "Step6 : loss = 9.453809976577759\n",
      "Step7 : loss = 9.422872066497803\n",
      "Step8 : loss = 9.401819229125977\n",
      "Step9 : loss = 9.388491153717041\n",
      "Data stream Batch- 1 : loss = 4.862168550491333\n",
      "Meta Update\n",
      "Training is starting\n",
      "Step 0 : loss = 1.9539155719210468\n",
      "Step 1 : loss = 1.9538966019445705\n",
      "Step 2 : loss = 1.9538770053949621\n",
      "Step 3 : loss = 1.9538578997173952\n",
      "Step 4 : loss = 1.953838527246955\n",
      "Step 5 : loss = 1.9538193026114077\n",
      "Step 6 : loss = 1.9537999586778736\n",
      "Step 7 : loss = 1.9537808390955131\n",
      "Step 8 : loss = 1.9537614949934539\n",
      "Step 9 : loss = 1.9537422730897864\n",
      "Update Procedure\n",
      "Step0 : loss = 10.49453846613566\n",
      "Step1 : loss = 10.405240853627523\n",
      "Step2 : loss = 10.313394546508789\n",
      "Step3 : loss = 10.233229478200277\n",
      "Step4 : loss = 10.16108218828837\n",
      "Step5 : loss = 10.098530451456705\n",
      "Step6 : loss = 10.047168493270874\n",
      "Step7 : loss = 10.00846274693807\n",
      "Step8 : loss = 9.981512467066446\n",
      "Step9 : loss = 9.9638565381368\n",
      "Data stream Batch- 2 : loss = 4.875917673110962\n",
      "Meta Update\n",
      "Training is starting\n",
      "Step 0 : loss = 1.9494493486448414\n",
      "Step 1 : loss = 1.9494279664748955\n",
      "Step 2 : loss = 1.9494066239290295\n",
      "Step 3 : loss = 1.949385156961424\n",
      "Step 4 : loss = 1.9493637779550184\n",
      "Step 5 : loss = 1.94934244963917\n",
      "Step 6 : loss = 1.949321081838201\n",
      "Step 7 : loss = 1.9492997538061843\n",
      "Step 8 : loss = 1.9492783744065534\n",
      "Step 9 : loss = 1.9492569242855387\n",
      "Update Procedure\n",
      "Step0 : loss = 10.113463699817657\n",
      "Step1 : loss = 9.986710667610168\n",
      "Step2 : loss = 9.860697448253632\n",
      "Step3 : loss = 9.749268114566803\n",
      "Step4 : loss = 9.653715133666992\n",
      "Step5 : loss = 9.570421874523163\n",
      "Step6 : loss = 9.504246711730957\n",
      "Step7 : loss = 9.45485109090805\n",
      "Step8 : loss = 9.420392751693726\n",
      "Step9 : loss = 9.39777398109436\n",
      "Data stream Batch- 3 : loss = 4.890638589859009\n",
      "Meta Update\n",
      "Training is starting\n",
      "Step 0 : loss = 1.9417068633295242\n",
      "Step 1 : loss = 1.9416819034469506\n",
      "Step 2 : loss = 1.941656810033416\n",
      "Step 3 : loss = 1.9416317310095543\n",
      "Step 4 : loss = 1.9416065950508392\n",
      "Step 5 : loss = 1.9415813800686645\n",
      "Step 6 : loss = 1.9415559207192725\n",
      "Step 7 : loss = 1.9415308446667734\n",
      "Step 8 : loss = 1.9415056297200777\n",
      "Step 9 : loss = 1.941480391762323\n",
      "Update Procedure\n",
      "Step0 : loss = 8.854928588867187\n",
      "Step1 : loss = 8.677375602722169\n",
      "Step2 : loss = 8.502934694290161\n",
      "Step3 : loss = 8.367563772201539\n",
      "Step4 : loss = 8.225872373580932\n",
      "Step5 : loss = 8.124231481552124\n",
      "Step6 : loss = 8.03695731163025\n",
      "Step7 : loss = 7.969067478179932\n",
      "Step8 : loss = 7.925505447387695\n",
      "Step9 : loss = 7.894858360290527\n",
      "Data stream Batch- 4 : loss = 4.919039964675903\n",
      "Meta Update\n",
      "Training is starting\n",
      "Step 0 : loss = 1.9282762540386074\n",
      "Step 1 : loss = 1.9282414807982389\n",
      "Step 2 : loss = 1.9282066818740633\n",
      "Step 3 : loss = 1.9281718006889736\n",
      "Step 4 : loss = 1.9281366698384756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5 : loss = 1.9281019962584924\n",
      "Step 6 : loss = 1.9280669731753213\n",
      "Step 7 : loss = 1.9280321743161906\n",
      "Step 8 : loss = 1.9279972679055635\n",
      "Step 9 : loss = 1.9279622249541775\n",
      "Update Procedure\n",
      "Step0 : loss = 7.297677993774414\n",
      "Step1 : loss = 7.136622230211894\n",
      "Step2 : loss = 6.987526933352153\n",
      "Step3 : loss = 6.853975335756938\n",
      "Step4 : loss = 6.736691157023112\n",
      "Step5 : loss = 6.640176057815552\n",
      "Step6 : loss = 6.550061821937561\n",
      "Step7 : loss = 6.492915193239848\n",
      "Step8 : loss = 6.44638987382253\n",
      "Step9 : loss = 6.418670157591502\n",
      "Data stream Batch- 5 : loss = 4.891485214233398\n",
      "Meta Update\n",
      "Training is starting\n",
      "Step 0 : loss = 1.9051689525238342\n",
      "Step 1 : loss = 1.9051172926164572\n",
      "Step 2 : loss = 1.9050657521726357\n",
      "Step 3 : loss = 1.9050141095996849\n",
      "Step 4 : loss = 1.9049624440748067\n",
      "Step 5 : loss = 1.904910804286954\n",
      "Step 6 : loss = 1.9048591448289771\n",
      "Step 7 : loss = 1.9048074630251715\n",
      "Step 8 : loss = 1.9047559202722613\n",
      "Step 9 : loss = 1.9047038981691002\n",
      "Update Procedure\n",
      "Step0 : loss = 6.166072879518781\n",
      "Step1 : loss = 6.019462483269828\n",
      "Step2 : loss = 5.870445234434945\n",
      "Step3 : loss = 5.727820788111005\n",
      "Step4 : loss = 5.601162433624268\n",
      "Step5 : loss = 5.487586430140904\n",
      "Step6 : loss = 5.401137045451573\n",
      "Step7 : loss = 5.332309552601406\n",
      "Step8 : loss = 5.284475105149405\n",
      "Step9 : loss = 5.252915212086269\n",
      "Data stream Batch- 6 : loss = 4.80893087387085\n",
      "Meta Update\n",
      "Training is starting\n",
      "Step 0 : loss = 1.8721533710107443\n",
      "Step 1 : loss = 1.872077237004562\n",
      "Step 2 : loss = 1.872001111820813\n",
      "Step 3 : loss = 1.8719246464116233\n",
      "Step 4 : loss = 1.871848411716166\n",
      "Step 5 : loss = 1.8717723021017654\n",
      "Step 6 : loss = 1.8716956929614146\n",
      "Step 7 : loss = 1.8716194416381537\n",
      "Step 8 : loss = 1.8715431821133408\n",
      "Step 9 : loss = 1.8714664542071877\n",
      "Update Procedure\n",
      "Step0 : loss = 5.209847494959831\n",
      "Step1 : loss = 5.051133796572685\n",
      "Step2 : loss = 4.898086428642273\n",
      "Step3 : loss = 4.752919912338257\n",
      "Step4 : loss = 4.61501157283783\n",
      "Step5 : loss = 4.506916269659996\n",
      "Step6 : loss = 4.422289401292801\n",
      "Step7 : loss = 4.358087003231049\n",
      "Step8 : loss = 4.313849583268166\n",
      "Step9 : loss = 4.284951612353325\n",
      "Data stream Batch- 7 : loss = 4.639097213745117\n",
      "Task  3\n",
      "Meta Update\n",
      "Training is starting\n",
      "Step 0 : loss = 0.4117512555449621\n",
      "Step 1 : loss = 0.41168078271261943\n",
      "Step 2 : loss = 0.4116099147096548\n",
      "Step 3 : loss = 0.4115393366556519\n",
      "Step 4 : loss = 0.4114683769152574\n",
      "Step 5 : loss = 0.41139733373183446\n",
      "Step 6 : loss = 0.41132632923764983\n",
      "Step 7 : loss = 0.4112552597294439\n",
      "Step 8 : loss = 0.4111840565677878\n",
      "Step 9 : loss = 0.41111283463726617\n",
      "Update Procedure\n",
      "Step0 : loss = 9.839766502380371\n",
      "Step1 : loss = 9.68648910522461\n",
      "Step2 : loss = 9.5355806350708\n",
      "Step3 : loss = 9.394197463989258\n",
      "Step4 : loss = 9.267868041992188\n",
      "Step5 : loss = 9.160704612731934\n",
      "Step6 : loss = 9.07497501373291\n",
      "Step7 : loss = 9.011397361755371\n",
      "Step8 : loss = 8.967206001281738\n",
      "Step9 : loss = 8.938752174377441\n",
      "Data stream Batch- 0 : loss = 8.13174057006836\n",
      "Meta Update\n",
      "Training is starting\n",
      "Step 0 : loss = 0.4016450256874006\n",
      "Step 1 : loss = 0.40155393100084574\n",
      "Step 2 : loss = 0.40146282549234535\n",
      "Step 3 : loss = 0.401371639819513\n",
      "Step 4 : loss = 0.4012804465598267\n",
      "Step 5 : loss = 0.4011892930424052\n",
      "Step 6 : loss = 0.4010981677103631\n",
      "Step 7 : loss = 0.401007073442089\n",
      "Step 8 : loss = 0.4009158531160229\n",
      "Step 9 : loss = 0.400824626672035\n",
      "Update Procedure\n",
      "Step0 : loss = 9.120154857635498\n",
      "Step1 : loss = 8.923984050750732\n",
      "Step2 : loss = 8.735549926757812\n",
      "Step3 : loss = 8.559664249420166\n",
      "Step4 : loss = 8.424944400787354\n",
      "Step5 : loss = 8.309637784957886\n",
      "Step6 : loss = 8.218123435974121\n",
      "Step7 : loss = 8.150365114212036\n",
      "Step8 : loss = 8.103852033615112\n",
      "Step9 : loss = 8.07363486289978\n",
      "Data stream Batch- 1 : loss = 7.7704691886901855\n",
      "Meta Update\n",
      "Training is starting\n",
      "Step 0 : loss = 0.38885705674215365\n",
      "Step 1 : loss = 0.3887502173712603\n",
      "Step 2 : loss = 0.3886433295946803\n",
      "Step 3 : loss = 0.3885365583234191\n",
      "Step 4 : loss = 0.3884297400872916\n",
      "Step 5 : loss = 0.38832284153980795\n",
      "Step 6 : loss = 0.3882158956539878\n",
      "Step 7 : loss = 0.3881088194321954\n",
      "Step 8 : loss = 0.38800170335803835\n",
      "Step 9 : loss = 0.3878945038817896\n",
      "Update Procedure\n",
      "Step0 : loss = 7.027187347412109\n",
      "Step1 : loss = 6.812787850697835\n",
      "Step2 : loss = 6.611971537272136\n",
      "Step3 : loss = 6.444167296091716\n",
      "Step4 : loss = 6.307206153869629\n",
      "Step5 : loss = 6.193458716074626\n",
      "Step6 : loss = 6.1038743654886884\n",
      "Step7 : loss = 6.037110964457194\n",
      "Step8 : loss = 5.990853945414226\n",
      "Step9 : loss = 5.960616111755371\n",
      "Data stream Batch- 2 : loss = 7.204154968261719\n",
      "Meta Update\n",
      "Training is starting\n",
      "Step 0 : loss = 0.3805663251801254\n",
      "Step 1 : loss = 0.3804557066012025\n",
      "Step 2 : loss = 0.3803449216720505\n",
      "Step 3 : loss = 0.38023410013317943\n",
      "Step 4 : loss = 0.38012322387311487\n",
      "Step 5 : loss = 0.3800121919420065\n",
      "Step 6 : loss = 0.37990114506784073\n",
      "Step 7 : loss = 0.3797901770850172\n",
      "Step 8 : loss = 0.3796791256586656\n",
      "Step 9 : loss = 0.37956802293464037\n",
      "Update Procedure\n",
      "Step0 : loss = 5.548204779624939\n",
      "Step1 : loss = 5.329606652259827\n",
      "Step2 : loss = 5.14862847328186\n",
      "Step3 : loss = 4.98766303062439\n",
      "Step4 : loss = 4.853813707828522\n",
      "Step5 : loss = 4.745010554790497\n",
      "Step6 : loss = 4.659195423126221\n",
      "Step7 : loss = 4.600150942802429\n",
      "Step8 : loss = 4.561160981655121\n",
      "Step9 : loss = 4.536257266998291\n",
      "Data stream Batch- 3 : loss = 6.391068696975708\n",
      "Meta Update\n",
      "Training is starting\n",
      "Step 0 : loss = 0.3744825063763321\n",
      "Step 1 : loss = 0.374366193034144\n",
      "Step 2 : loss = 0.37424987143180743\n",
      "Step 3 : loss = 0.3741333718402536\n",
      "Step 4 : loss = 0.37401682502075606\n",
      "Step 5 : loss = 0.37390014380982667\n",
      "Step 6 : loss = 0.37378341148354877\n",
      "Step 7 : loss = 0.37366654482140876\n",
      "Step 8 : loss = 0.37354962672901365\n",
      "Step 9 : loss = 0.3734325855960463\n",
      "Update Procedure\n",
      "Step0 : loss = 4.641556882858277\n",
      "Step1 : loss = 4.495230865478516\n",
      "Step2 : loss = 4.374352169036865\n",
      "Step3 : loss = 4.272446393966675\n",
      "Step4 : loss = 4.191502285003662\n",
      "Step5 : loss = 4.121573781967163\n",
      "Step6 : loss = 4.066832208633423\n",
      "Step7 : loss = 4.023975372314453\n",
      "Step8 : loss = 3.9962369441986083\n",
      "Step9 : loss = 3.977723217010498\n",
      "Data stream Batch- 4 : loss = 5.534229516983032\n",
      "Meta Update\n",
      "Training is starting\n",
      "Step 0 : loss = 0.3725166841607843\n",
      "Step 1 : loss = 0.37240322759420175\n",
      "Step 2 : loss = 0.37228976674341285\n",
      "Step 3 : loss = 0.3721761789610571\n",
      "Step 4 : loss = 0.37206259115057905\n",
      "Step 5 : loss = 0.3719488763042355\n",
      "Step 6 : loss = 0.37183507450434233\n",
      "Step 7 : loss = 0.37172122871924923\n",
      "Step 8 : loss = 0.3716072566069897\n",
      "Step 9 : loss = 0.3714932355286682\n",
      "Update Procedure\n",
      "Step0 : loss = 4.457114617029826\n",
      "Step1 : loss = 4.34309200445811\n",
      "Step2 : loss = 4.233728011449178\n",
      "Step3 : loss = 4.136796792348226\n",
      "Step4 : loss = 4.049489001433055\n",
      "Step5 : loss = 3.9779043396313987\n",
      "Step6 : loss = 3.9218859473864236\n",
      "Step7 : loss = 3.8810818592707315\n",
      "Step8 : loss = 3.8530386288960776\n",
      "Step9 : loss = 3.8351083000501\n",
      "Data stream Batch- 5 : loss = 4.853086233139038\n",
      "Meta Update\n",
      "Training is starting\n",
      "Step 0 : loss = 0.38188064332917254\n",
      "Step 1 : loss = 0.3817770392842657\n",
      "Step 2 : loss = 0.38167338380867816\n",
      "Step 3 : loss = 0.38156964882143213\n",
      "Step 4 : loss = 0.38146575384944287\n",
      "Step 5 : loss = 0.38136181134148023\n",
      "Step 6 : loss = 0.38125777910867226\n",
      "Step 7 : loss = 0.3811537034363952\n",
      "Step 8 : loss = 0.38104948975525493\n",
      "Step 9 : loss = 0.3809453119199018\n",
      "Update Procedure\n",
      "Step0 : loss = 4.163396034921918\n",
      "Step1 : loss = 4.05929970741272\n",
      "Step2 : loss = 3.944549356188093\n",
      "Step3 : loss = 3.8434708203588213\n",
      "Step4 : loss = 3.7671698416982378\n",
      "Step5 : loss = 3.7087408048766\n",
      "Step6 : loss = 3.665245941707066\n",
      "Step7 : loss = 3.626044205256871\n",
      "Step8 : loss = 3.6030461362430026\n",
      "Step9 : loss = 3.5867729357310703\n",
      "Data stream Batch- 6 : loss = 4.255273461341858\n",
      "Meta Update\n",
      "Training is starting\n",
      "Step 0 : loss = 0.39051405403320266\n",
      "Step 1 : loss = 0.39042357951799067\n",
      "Step 2 : loss = 0.3903330063814559\n",
      "Step 3 : loss = 0.39024238061328087\n",
      "Step 4 : loss = 0.3901516334472654\n",
      "Step 5 : loss = 0.3900607815465031\n",
      "Step 6 : loss = 0.3899696326097079\n",
      "Step 7 : loss = 0.38987842047856214\n",
      "Step 8 : loss = 0.3897871171380198\n",
      "Step 9 : loss = 0.38969574628850173\n",
      "Update Procedure\n",
      "Step0 : loss = 3.7746694162487984\n",
      "Step1 : loss = 3.6704639941453934\n",
      "Step2 : loss = 3.548061840236187\n",
      "Step3 : loss = 3.48022336140275\n",
      "Step4 : loss = 3.366504445672035\n",
      "Step5 : loss = 3.317869734019041\n",
      "Step6 : loss = 3.2560257092118263\n",
      "Step7 : loss = 3.2209548093378544\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step8 : loss = 3.1935577057302\n",
      "Step9 : loss = 3.175239372998476\n",
      "Data stream Batch- 7 : loss = 3.691208839416504\n",
      "Task  4\n",
      "Meta Update\n",
      "Training is starting\n",
      "Step 0 : loss = 0.25363506227105337\n",
      "Step 1 : loss = 0.25250970350562546\n",
      "Step 2 : loss = 0.25138222793238657\n",
      "Step 3 : loss = 0.250252658956366\n",
      "Step 4 : loss = 0.24912122863217756\n",
      "Step 5 : loss = 0.24798902585288077\n",
      "Step 6 : loss = 0.24685547820966314\n",
      "Step 7 : loss = 0.2457203881988504\n",
      "Step 8 : loss = 0.24458330670540776\n",
      "Step 9 : loss = 0.24344500051534582\n",
      "Update Procedure\n",
      "Step0 : loss = 2.0737593173980713\n",
      "Step1 : loss = 1.9861173629760742\n",
      "Step2 : loss = 1.9016419649124146\n",
      "Step3 : loss = 1.8248322010040283\n",
      "Step4 : loss = 1.7573657035827637\n",
      "Step5 : loss = 1.7007720470428467\n",
      "Step6 : loss = 1.655786156654358\n",
      "Step7 : loss = 1.6222829818725586\n",
      "Step8 : loss = 1.599492073059082\n",
      "Step9 : loss = 1.585053563117981\n",
      "Data stream Batch- 0 : loss = 6.83274245262146\n",
      "Meta Update\n",
      "Training is starting\n",
      "Step 0 : loss = 0.2622081420170795\n",
      "Step 1 : loss = 0.2610795754451193\n",
      "Step 2 : loss = 0.2599501505608293\n",
      "Step 3 : loss = 0.25881860505142124\n",
      "Step 4 : loss = 0.25768493889011257\n",
      "Step 5 : loss = 0.2565491393433696\n",
      "Step 6 : loss = 0.255411854996875\n",
      "Step 7 : loss = 0.2542732751365014\n",
      "Step 8 : loss = 0.2531329468945616\n",
      "Step 9 : loss = 0.25199157615651707\n",
      "Update Procedure\n",
      "Step0 : loss = 2.690013110637665\n",
      "Step1 : loss = 2.557462215423584\n",
      "Step2 : loss = 2.4385695457458496\n",
      "Step3 : loss = 2.3375170826911926\n",
      "Step4 : loss = 2.2559801936149597\n",
      "Step5 : loss = 2.1913838386535645\n",
      "Step6 : loss = 2.1420846581459045\n",
      "Step7 : loss = 2.1066063940525055\n",
      "Step8 : loss = 2.082834839820862\n",
      "Step9 : loss = 2.067276954650879\n",
      "Data stream Batch- 1 : loss = 6.8813560009002686\n",
      "Meta Update\n",
      "Training is starting\n",
      "Step 0 : loss = 0.3339457905859183\n",
      "Step 1 : loss = 0.33293851481049075\n",
      "Step 2 : loss = 0.3319293431779537\n",
      "Step 3 : loss = 0.3309183079396155\n",
      "Step 4 : loss = 0.32990537585606716\n",
      "Step 5 : loss = 0.32889048506848256\n",
      "Step 6 : loss = 0.327873597971405\n",
      "Step 7 : loss = 0.32685481671840644\n",
      "Step 8 : loss = 0.3258341065510027\n",
      "Step 9 : loss = 0.324811558988169\n",
      "Update Procedure\n",
      "Step0 : loss = 4.288738290468852\n",
      "Step1 : loss = 4.1370450258255005\n",
      "Step2 : loss = 3.9850460290908813\n",
      "Step3 : loss = 3.8641539414723716\n",
      "Step4 : loss = 3.7590103149414062\n",
      "Step5 : loss = 3.706844131151835\n",
      "Step6 : loss = 3.6495790084203086\n",
      "Step7 : loss = 3.6207900444666543\n",
      "Step8 : loss = 3.5977087020874023\n",
      "Step9 : loss = 3.5836777885754905\n",
      "Data stream Batch- 2 : loss = 6.623236179351807\n",
      "Meta Update\n",
      "Training is starting\n",
      "Step 0 : loss = 0.3725804765494437\n",
      "Step 1 : loss = 0.37163081077400467\n",
      "Step 2 : loss = 0.3706794908367534\n",
      "Step 3 : loss = 0.369726443341095\n",
      "Step 4 : loss = 0.3687715648510898\n",
      "Step 5 : loss = 0.367815066105738\n",
      "Step 6 : loss = 0.36685671054024577\n",
      "Step 7 : loss = 0.365896712902924\n",
      "Step 8 : loss = 0.3649343760414763\n",
      "Step 9 : loss = 0.36397079271622534\n",
      "Update Procedure\n",
      "Step0 : loss = 4.044996842741966\n",
      "Step1 : loss = 3.9481603801250458\n",
      "Step2 : loss = 3.8638657927513123\n",
      "Step3 : loss = 3.8006402105093002\n",
      "Step4 : loss = 3.736256420612335\n",
      "Step5 : loss = 3.6873376667499542\n",
      "Step6 : loss = 3.647986501455307\n",
      "Step7 : loss = 3.6187988072633743\n",
      "Step8 : loss = 3.598692812025547\n",
      "Step9 : loss = 3.5849951207637787\n",
      "Data stream Batch- 3 : loss = 6.4525628089904785\n",
      "Meta Update\n",
      "Training is starting\n",
      "Step 0 : loss = 0.3834680460466747\n",
      "Step 1 : loss = 0.3825274306934535\n",
      "Step 2 : loss = 0.38158529220702364\n",
      "Step 3 : loss = 0.3806418135907019\n",
      "Step 4 : loss = 0.37969645415816455\n",
      "Step 5 : loss = 0.378749204718261\n",
      "Step 6 : loss = 0.37780016004953243\n",
      "Step 7 : loss = 0.3768495273263006\n",
      "Step 8 : loss = 0.3758974975905521\n",
      "Step 9 : loss = 0.3749431113564395\n",
      "Update Procedure\n",
      "Step0 : loss = 5.030504232645034\n",
      "Step1 : loss = 4.8970124363899235\n",
      "Step2 : loss = 4.7658889353275296\n",
      "Step3 : loss = 4.671852850914002\n",
      "Step4 : loss = 4.5835227310657505\n",
      "Step5 : loss = 4.506833213567734\n",
      "Step6 : loss = 4.4417864382267\n",
      "Step7 : loss = 4.394466364383698\n",
      "Step8 : loss = 4.359308177232743\n",
      "Step9 : loss = 4.337433010339737\n",
      "Data stream Batch- 4 : loss = 6.205577611923218\n",
      "Meta Update\n",
      "Training is starting\n",
      "Step 0 : loss = 0.37589103658120865\n",
      "Step 1 : loss = 0.37489323882073305\n",
      "Step 2 : loss = 0.3738937603505632\n",
      "Step 3 : loss = 0.37289236163936545\n",
      "Step 4 : loss = 0.37188922911049044\n",
      "Step 5 : loss = 0.37088350906991424\n",
      "Step 6 : loss = 0.36987572161489685\n",
      "Step 7 : loss = 0.3688662657878755\n",
      "Step 8 : loss = 0.36785486204127255\n",
      "Step 9 : loss = 0.3668417532852294\n",
      "Update Procedure\n",
      "Step0 : loss = 4.619675810138385\n",
      "Step1 : loss = 4.471697370211284\n",
      "Step2 : loss = 4.317304829756419\n",
      "Step3 : loss = 4.187458937366803\n",
      "Step4 : loss = 4.069456120332082\n",
      "Step5 : loss = 3.9670681605736413\n",
      "Step6 : loss = 3.8946473052104316\n",
      "Step7 : loss = 3.83607525130113\n",
      "Step8 : loss = 3.796512653430303\n",
      "Step9 : loss = 3.7707534631093345\n",
      "Data stream Batch- 5 : loss = 5.7412109375\n",
      "Meta Update\n",
      "Training is starting\n",
      "Step 0 : loss = 0.35402148676888867\n",
      "Step 1 : loss = 0.35294612819511606\n",
      "Step 2 : loss = 0.3518712405000966\n",
      "Step 3 : loss = 0.35079450207149937\n",
      "Step 4 : loss = 0.34971638778728475\n",
      "Step 5 : loss = 0.3486424350265679\n",
      "Step 6 : loss = 0.3475847527794395\n",
      "Step 7 : loss = 0.34652648854874685\n",
      "Step 8 : loss = 0.3454674389522803\n",
      "Step 9 : loss = 0.34440828814252566\n",
      "Update Procedure\n",
      "Step0 : loss = 3.8961800805159976\n",
      "Step1 : loss = 3.766057406152998\n",
      "Step2 : loss = 3.5877577705042705\n",
      "Step3 : loss = 3.4865598337990895\n",
      "Step4 : loss = 3.3656655081680844\n",
      "Step5 : loss = 3.2703774315970287\n",
      "Step6 : loss = 3.2040083834103177\n",
      "Step7 : loss = 3.1493635220187053\n",
      "Step8 : loss = 3.1154477851731435\n",
      "Step9 : loss = 3.092217892408371\n",
      "Data stream Batch- 6 : loss = 4.905255079269409\n",
      "Meta Update\n",
      "Training is starting\n",
      "Step 0 : loss = 0.3730741332686728\n",
      "Step 1 : loss = 0.3722776274163421\n",
      "Step 2 : loss = 0.37149857236861006\n",
      "Step 3 : loss = 0.3707228283386575\n",
      "Step 4 : loss = 0.36995845278250483\n",
      "Step 5 : loss = 0.36919251614740767\n",
      "Step 6 : loss = 0.36842489739915263\n",
      "Step 7 : loss = 0.3676559762800625\n",
      "Step 8 : loss = 0.3668871567846476\n",
      "Step 9 : loss = 0.3661237009920788\n",
      "Update Procedure\n",
      "Step0 : loss = 3.085676733404398\n",
      "Step1 : loss = 2.9638736471533775\n",
      "Step2 : loss = 2.8400313518941402\n",
      "Step3 : loss = 2.7328705601394176\n",
      "Step4 : loss = 2.6320518851280212\n",
      "Step5 : loss = 2.5502322390675545\n",
      "Step6 : loss = 2.486558422446251\n",
      "Step7 : loss = 2.4394930116832256\n",
      "Step8 : loss = 2.4078993797302246\n",
      "Step9 : loss = 2.3872068487107754\n",
      "Data stream Batch- 7 : loss = 3.8738155364990234\n",
      "Task  5\n",
      "Meta Update\n",
      "Training is starting\n",
      "Step 0 : loss = 0.11578924744093452\n",
      "Step 1 : loss = 0.11553272607774738\n",
      "Step 2 : loss = 0.11526063311939302\n",
      "Step 3 : loss = 0.11498841323805664\n",
      "Step 4 : loss = 0.11471713221119735\n",
      "Step 5 : loss = 0.11444701469002747\n",
      "Step 6 : loss = 0.11417686959652473\n",
      "Step 7 : loss = 0.11390666497964153\n",
      "Step 8 : loss = 0.11363643833282475\n",
      "Step 9 : loss = 0.11336617478600623\n",
      "Update Procedure\n",
      "Step0 : loss = 2.7203915119171143\n",
      "Step1 : loss = 2.432107448577881\n",
      "Step2 : loss = 2.156174898147583\n",
      "Step3 : loss = 1.906830906867981\n",
      "Step4 : loss = 1.7106764316558838\n",
      "Step5 : loss = 1.5583126544952393\n",
      "Step6 : loss = 1.4419066905975342\n",
      "Step7 : loss = 1.3607388734817505\n",
      "Step8 : loss = 1.309799313545227\n",
      "Step9 : loss = 1.2807272672653198\n",
      "Data stream Batch- 0 : loss = 3.8317357301712036\n",
      "Meta Update\n",
      "Training is starting\n",
      "Step 0 : loss = 0.09646881792845215\n",
      "Step 1 : loss = 0.09630580217975078\n",
      "Step 2 : loss = 0.09614401780317934\n",
      "Step 3 : loss = 0.09598346949613223\n",
      "Step 4 : loss = 0.0958229218053065\n",
      "Step 5 : loss = 0.09566236818724562\n",
      "Step 6 : loss = 0.09550181465244739\n",
      "Step 7 : loss = 0.0953412539244864\n",
      "Step 8 : loss = 0.09518071240681517\n",
      "Step 9 : loss = 0.09502015773522636\n",
      "Update Procedure\n",
      "Step0 : loss = 1.7947764992713928\n",
      "Step1 : loss = 1.669027864933014\n",
      "Step2 : loss = 1.4483423829078674\n",
      "Step3 : loss = 1.4171244502067566\n",
      "Step4 : loss = 1.3115477561950684\n",
      "Step5 : loss = 1.2905997335910797\n",
      "Step6 : loss = 1.2613351345062256\n",
      "Step7 : loss = 1.2349900305271149\n",
      "Step8 : loss = 1.217305064201355\n",
      "Step9 : loss = 1.2067539691925049\n",
      "Data stream Batch- 1 : loss = 3.6400647163391113\n",
      "Meta Update\n",
      "Training is starting\n",
      "Step 0 : loss = 0.08723309298813878\n",
      "Step 1 : loss = 0.08707812678611301\n",
      "Step 2 : loss = 0.08692318987508671\n",
      "Step 3 : loss = 0.08677093449411136\n",
      "Step 4 : loss = 0.08661867849180485\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5 : loss = 0.08646640465739695\n",
      "Step 6 : loss = 0.08631415469443164\n",
      "Step 7 : loss = 0.08616188088807993\n",
      "Step 8 : loss = 0.08600961938009062\n",
      "Step 9 : loss = 0.08585735327710735\n",
      "Update Procedure\n",
      "Step0 : loss = 2.036848505338033\n",
      "Step1 : loss = 1.9724860588709514\n",
      "Step2 : loss = 1.917013903458913\n",
      "Step3 : loss = 1.865784744421641\n",
      "Step4 : loss = 1.814286967118581\n",
      "Step5 : loss = 1.7760268052419026\n",
      "Step6 : loss = 1.7416317065556843\n",
      "Step7 : loss = 1.719014048576355\n",
      "Step8 : loss = 1.6996171077092488\n",
      "Step9 : loss = 1.6882490515708923\n",
      "Data stream Batch- 2 : loss = 3.5620003938674927\n",
      "Meta Update\n",
      "Training is starting\n",
      "Step 0 : loss = 0.08537713900935824\n",
      "Step 1 : loss = 0.08521615048939066\n",
      "Step 2 : loss = 0.08505513154579185\n",
      "Step 3 : loss = 0.08489413084581213\n",
      "Step 4 : loss = 0.0847331137227769\n",
      "Step 5 : loss = 0.08457213201062172\n",
      "Step 6 : loss = 0.08441751574884832\n",
      "Step 7 : loss = 0.0842565230882781\n",
      "Step 8 : loss = 0.08409552062623535\n",
      "Step 9 : loss = 0.08393449959900942\n",
      "Update Procedure\n",
      "Step0 : loss = 2.61898934841156\n",
      "Step1 : loss = 2.549642860889435\n",
      "Step2 : loss = 2.479012653231621\n",
      "Step3 : loss = 2.4222034215927124\n",
      "Step4 : loss = 2.357948511838913\n",
      "Step5 : loss = 2.3117759227752686\n",
      "Step6 : loss = 2.2666634172201157\n",
      "Step7 : loss = 2.2365506291389465\n",
      "Step8 : loss = 2.2124511897563934\n",
      "Step9 : loss = 2.1983579248189926\n",
      "Data stream Batch- 3 : loss = 3.4656474590301514\n",
      "Meta Update\n",
      "Training is starting\n",
      "Step 0 : loss = 0.07700131829138698\n",
      "Step 1 : loss = 0.07685852458216401\n",
      "Step 2 : loss = 0.0767167807844141\n",
      "Step 3 : loss = 0.07657757782897039\n",
      "Step 4 : loss = 0.07644054568040262\n",
      "Step 5 : loss = 0.0763034950666887\n",
      "Step 6 : loss = 0.07616645172898964\n",
      "Step 7 : loss = 0.07602939583374543\n",
      "Step 8 : loss = 0.07589321680981256\n",
      "Step 9 : loss = 0.07575975925667403\n",
      "Update Procedure\n",
      "Step0 : loss = 2.4661412119865416\n",
      "Step1 : loss = 2.375374400615692\n",
      "Step2 : loss = 2.2762844443321226\n",
      "Step3 : loss = 2.201913928985596\n",
      "Step4 : loss = 2.1242788076400756\n",
      "Step5 : loss = 2.051753115653992\n",
      "Step6 : loss = 2.009579288959503\n",
      "Step7 : loss = 1.9706480383872986\n",
      "Step8 : loss = 1.9433117151260375\n",
      "Step9 : loss = 1.926207137107849\n",
      "Data stream Batch- 4 : loss = 3.2787030935287476\n",
      "Meta Update\n",
      "Training is starting\n",
      "Step 0 : loss = 0.0730377987580478\n",
      "Step 1 : loss = 0.07293354795462258\n",
      "Step 2 : loss = 0.07282930309564291\n",
      "Step 3 : loss = 0.07272506018916611\n",
      "Step 4 : loss = 0.07262082198866027\n",
      "Step 5 : loss = 0.07251657911163116\n",
      "Step 6 : loss = 0.07241233628770881\n",
      "Step 7 : loss = 0.07230808808832259\n",
      "Step 8 : loss = 0.07220383992621852\n",
      "Step 9 : loss = 0.07209959905782519\n",
      "Update Procedure\n",
      "Step0 : loss = 2.4070796072483063\n",
      "Step1 : loss = 2.323684831460317\n",
      "Step2 : loss = 2.2276484171549478\n",
      "Step3 : loss = 2.153782546520233\n",
      "Step4 : loss = 2.075794075926145\n",
      "Step5 : loss = 2.032920867204666\n",
      "Step6 : loss = 1.9986052562793095\n",
      "Step7 : loss = 1.9703892717758815\n",
      "Step8 : loss = 1.9561253537734349\n",
      "Step9 : loss = 1.9435570190350215\n",
      "Data stream Batch- 5 : loss = 3.092334270477295\n",
      "Meta Update\n",
      "Training is starting\n",
      "Step 0 : loss = 0.0718930215489435\n",
      "Step 1 : loss = 0.07176555929986964\n",
      "Step 2 : loss = 0.0716394304811038\n",
      "Step 3 : loss = 0.07151680321720483\n",
      "Step 4 : loss = 0.07139624158923061\n",
      "Step 5 : loss = 0.0712756754507791\n",
      "Step 6 : loss = 0.07115510383974422\n",
      "Step 7 : loss = 0.07103453238234224\n",
      "Step 8 : loss = 0.07091220230465424\n",
      "Step 9 : loss = 0.07079180721365116\n",
      "Update Procedure\n",
      "Step0 : loss = 2.000608512333461\n",
      "Step1 : loss = 1.955564865044185\n",
      "Step2 : loss = 1.9008486994675227\n",
      "Step3 : loss = 1.8798192228589738\n",
      "Step4 : loss = 1.8227902778557368\n",
      "Step5 : loss = 1.8082621140139443\n",
      "Step6 : loss = 1.7693134248256683\n",
      "Step7 : loss = 1.7613051193101066\n",
      "Step8 : loss = 1.746694415807724\n",
      "Step9 : loss = 1.738277439560209\n",
      "Data stream Batch- 6 : loss = 2.996687889099121\n",
      "Meta Update\n",
      "Training is starting\n",
      "Step 0 : loss = 0.08045612483537438\n",
      "Step 1 : loss = 0.08027127102015326\n",
      "Step 2 : loss = 0.08008605013590915\n",
      "Step 3 : loss = 0.07990082129646736\n",
      "Step 4 : loss = 0.07971560593387944\n",
      "Step 5 : loss = 0.07953173543640704\n",
      "Step 6 : loss = 0.07934861788773317\n",
      "Step 7 : loss = 0.0791679373157129\n",
      "Step 8 : loss = 0.07898779444917303\n",
      "Step 9 : loss = 0.07881084924657573\n",
      "Update Procedure\n",
      "Step0 : loss = 1.9107463024556637\n",
      "Step1 : loss = 1.9217130206525326\n",
      "Step2 : loss = 1.8287882506847382\n",
      "Step3 : loss = 1.8371410891413689\n",
      "Step4 : loss = 1.7664086259901524\n",
      "Step5 : loss = 1.7540959417819977\n",
      "Step6 : loss = 1.7488034721463919\n",
      "Step7 : loss = 1.7325968779623508\n",
      "Step8 : loss = 1.6987127996981144\n",
      "Step9 : loss = 1.6915189772844315\n",
      "Data stream Batch- 7 : loss = 2.9243974685668945\n",
      "Task  6\n",
      "Meta Update\n",
      "Training is starting\n",
      "Step 0 : loss = 0.12559181385027537\n",
      "Step 1 : loss = 0.12499642729676731\n",
      "Step 2 : loss = 0.12441293185583789\n",
      "Step 3 : loss = 0.1238467167694557\n",
      "Step 4 : loss = 0.12329024520353692\n",
      "Step 5 : loss = 0.12273988085774841\n",
      "Step 6 : loss = 0.1221893872691946\n",
      "Step 7 : loss = 0.12164292668127047\n",
      "Step 8 : loss = 0.12110166583114072\n",
      "Step 9 : loss = 0.12056025306468034\n",
      "Update Procedure\n",
      "Step0 : loss = 3.333542585372925\n",
      "Step1 : loss = 3.2827563285827637\n",
      "Step2 : loss = 3.2334792613983154\n",
      "Step3 : loss = 3.187814712524414\n",
      "Step4 : loss = 3.1474716663360596\n",
      "Step5 : loss = 3.1137022972106934\n",
      "Step6 : loss = 3.086937665939331\n",
      "Step7 : loss = 3.067166805267334\n",
      "Step8 : loss = 3.0535082817077637\n",
      "Step9 : loss = 3.0446572303771973\n",
      "Data stream Batch- 0 : loss = 11.19450330734253\n",
      "Meta Update\n",
      "Training is starting\n",
      "Step 0 : loss = 0.13750179798873227\n",
      "Step 1 : loss = 0.13698480688322764\n",
      "Step 2 : loss = 0.13646860454689588\n",
      "Step 3 : loss = 0.13595633369147594\n",
      "Step 4 : loss = 0.13544822810096632\n",
      "Step 5 : loss = 0.13494200489036118\n",
      "Step 6 : loss = 0.13443556035976453\n",
      "Step 7 : loss = 0.1339301459556553\n",
      "Step 8 : loss = 0.1334267878765637\n",
      "Step 9 : loss = 0.1329232003853298\n",
      "Update Procedure\n",
      "Step0 : loss = 3.618369460105896\n",
      "Step1 : loss = 3.528867721557617\n",
      "Step2 : loss = 3.4435700178146362\n",
      "Step3 : loss = 3.369796633720398\n",
      "Step4 : loss = 3.306992292404175\n",
      "Step5 : loss = 3.255392909049988\n",
      "Step6 : loss = 3.2147072553634644\n",
      "Step7 : loss = 3.1849007606506348\n",
      "Step8 : loss = 3.1645424365997314\n",
      "Step9 : loss = 3.151373505592346\n",
      "Data stream Batch- 1 : loss = 10.712769985198975\n",
      "Meta Update\n",
      "Training is starting\n",
      "Step 0 : loss = 0.17550969924003798\n",
      "Step 1 : loss = 0.17487663674553247\n",
      "Step 2 : loss = 0.17424330588105838\n",
      "Step 3 : loss = 0.17360968801542842\n",
      "Step 4 : loss = 0.17297576234921833\n",
      "Step 5 : loss = 0.17234157442242568\n",
      "Step 6 : loss = 0.17170709485772764\n",
      "Step 7 : loss = 0.171072328145761\n",
      "Step 8 : loss = 0.1704373885078028\n",
      "Step 9 : loss = 0.16980359026009795\n",
      "Update Procedure\n",
      "Step0 : loss = 4.173560460408528\n",
      "Step1 : loss = 4.068859577178955\n",
      "Step2 : loss = 4.005104621251424\n",
      "Step3 : loss = 3.9518706798553467\n",
      "Step4 : loss = 3.9039040406545005\n",
      "Step5 : loss = 3.8631173769632974\n",
      "Step6 : loss = 3.8301614920298257\n",
      "Step7 : loss = 3.805474281311035\n",
      "Step8 : loss = 3.7883376280466714\n",
      "Step9 : loss = 3.777064879735311\n",
      "Data stream Batch- 2 : loss = 10.065764427185059\n",
      "Meta Update\n",
      "Training is starting\n",
      "Step 0 : loss = 0.21259795209939827\n",
      "Step 1 : loss = 0.21195185894489021\n",
      "Step 2 : loss = 0.2113054781307916\n",
      "Step 3 : loss = 0.2106588089906127\n",
      "Step 4 : loss = 0.21001185196374836\n",
      "Step 5 : loss = 0.20936458775988762\n",
      "Step 6 : loss = 0.20871705512388572\n",
      "Step 7 : loss = 0.20806918134906258\n",
      "Step 8 : loss = 0.20742108223057662\n",
      "Step 9 : loss = 0.20677268559697803\n",
      "Update Procedure\n",
      "Step0 : loss = 4.144979119300842\n",
      "Step1 : loss = 4.06283974647522\n",
      "Step2 : loss = 3.9775436520576477\n",
      "Step3 : loss = 3.89441579580307\n",
      "Step4 : loss = 3.8188539147377014\n",
      "Step5 : loss = 3.7540979385375977\n",
      "Step6 : loss = 3.7020254135131836\n",
      "Step7 : loss = 3.663486212491989\n",
      "Step8 : loss = 3.6365597248077393\n",
      "Step9 : loss = 3.6189514696598053\n",
      "Data stream Batch- 3 : loss = 9.11952018737793\n",
      "Meta Update\n",
      "Training is starting\n",
      "Step 0 : loss = 0.25380573684782193\n",
      "Step 1 : loss = 0.2531407250281535\n",
      "Step 2 : loss = 0.2524754053691362\n",
      "Step 3 : loss = 0.2518098590682475\n",
      "Step 4 : loss = 0.25114400582936497\n",
      "Step 5 : loss = 0.2504778845238713\n",
      "Step 6 : loss = 0.24982601104893354\n",
      "Step 7 : loss = 0.24916054817757058\n",
      "Step 8 : loss = 0.2484974842418012\n",
      "Step 9 : loss = 0.24783415220449365\n",
      "Update Procedure\n",
      "Step0 : loss = 3.7977417945861816\n",
      "Step1 : loss = 3.6684186935424803\n",
      "Step2 : loss = 3.535507583618164\n",
      "Step3 : loss = 3.414673686027527\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step4 : loss = 3.303312301635742\n",
      "Step5 : loss = 3.2115934610366823\n",
      "Step6 : loss = 3.136937475204468\n",
      "Step7 : loss = 3.0811914205551147\n",
      "Step8 : loss = 3.04258451461792\n",
      "Step9 : loss = 3.0173590183258057\n",
      "Data stream Batch- 4 : loss = 7.790063142776489\n",
      "Meta Update\n",
      "Training is starting\n",
      "Step 0 : loss = 0.30429455054235627\n",
      "Step 1 : loss = 0.30359813629179533\n",
      "Step 2 : loss = 0.3029014357564798\n",
      "Step 3 : loss = 0.302204509521618\n",
      "Step 4 : loss = 0.30153147691710336\n",
      "Step 5 : loss = 0.30085830315548207\n",
      "Step 6 : loss = 0.30016070190952454\n",
      "Step 7 : loss = 0.2994628722559041\n",
      "Step 8 : loss = 0.29876473833463885\n",
      "Step 9 : loss = 0.29806639595509127\n",
      "Update Procedure\n",
      "Step0 : loss = 3.179921785990397\n",
      "Step1 : loss = 3.0490514636039734\n",
      "Step2 : loss = 2.9227894246578217\n",
      "Step3 : loss = 2.802166293064753\n",
      "Step4 : loss = 2.698548063635826\n",
      "Step5 : loss = 2.61163891851902\n",
      "Step6 : loss = 2.5486236910025277\n",
      "Step7 : loss = 2.5069559812545776\n",
      "Step8 : loss = 2.4799766639868417\n",
      "Step9 : loss = 2.4632561951875687\n",
      "Data stream Batch- 5 : loss = 6.639022350311279\n",
      "Meta Update\n",
      "Training is starting\n",
      "Step 0 : loss = 0.34011768187615926\n",
      "Step 1 : loss = 0.3393908817659286\n",
      "Step 2 : loss = 0.33866391469911866\n",
      "Step 3 : loss = 0.3379367020651263\n",
      "Step 4 : loss = 0.337209304411407\n",
      "Step 5 : loss = 0.33648160237161445\n",
      "Step 6 : loss = 0.33575373243174284\n",
      "Step 7 : loss = 0.3350256186528038\n",
      "Step 8 : loss = 0.334297237789829\n",
      "Step 9 : loss = 0.33358912378571387\n",
      "Update Procedure\n",
      "Step0 : loss = 2.7199519276618958\n",
      "Step1 : loss = 2.6110348403453827\n",
      "Step2 : loss = 2.5162452459335327\n",
      "Step3 : loss = 2.426354910646166\n",
      "Step4 : loss = 2.3568945356777737\n",
      "Step5 : loss = 2.2966834349291667\n",
      "Step6 : loss = 2.2521991303988864\n",
      "Step7 : loss = 2.2184378589902605\n",
      "Step8 : loss = 2.194827492747988\n",
      "Step9 : loss = 2.1806456787245616\n",
      "Data stream Batch- 6 : loss = 6.069361686706543\n",
      "Meta Update\n",
      "Training is starting\n",
      "Step 0 : loss = 0.29793732449238214\n",
      "Step 1 : loss = 0.29723527920084364\n",
      "Step 2 : loss = 0.2965140071947026\n",
      "Step 3 : loss = 0.29580979829553716\n",
      "Step 4 : loss = 0.29508803338618916\n",
      "Step 5 : loss = 0.29436600213967\n",
      "Step 6 : loss = 0.2936437242663441\n",
      "Step 7 : loss = 0.29292120110696\n",
      "Step 8 : loss = 0.29219839100406064\n",
      "Step 9 : loss = 0.29147537510839355\n",
      "Update Procedure\n",
      "Step0 : loss = 2.315927967429161\n",
      "Step1 : loss = 2.2421518228948116\n",
      "Step2 : loss = 2.157711487263441\n",
      "Step3 : loss = 2.0837679468095303\n",
      "Step4 : loss = 2.0163330361247063\n",
      "Step5 : loss = 1.9625683017075062\n",
      "Step6 : loss = 1.920120645314455\n",
      "Step7 : loss = 1.8872628770768642\n",
      "Step8 : loss = 1.8640881665050983\n",
      "Step9 : loss = 1.8490227237343788\n",
      "Data stream Batch- 7 : loss = 5.585177183151245\n",
      "Task  7\n",
      "Meta Update\n",
      "Training is starting\n",
      "Step 0 : loss = 0.12246331554207072\n",
      "Step 1 : loss = 0.12229734826765151\n",
      "Step 2 : loss = 0.12212967400878515\n",
      "Step 3 : loss = 0.12196400515914739\n",
      "Step 4 : loss = 0.12179827702880777\n",
      "Step 5 : loss = 0.12163249815982767\n",
      "Step 6 : loss = 0.1214645903948688\n",
      "Step 7 : loss = 0.12129870908186847\n",
      "Step 8 : loss = 0.12113277735897059\n",
      "Step 9 : loss = 0.12096680247138362\n",
      "Update Procedure\n",
      "Step0 : loss = 8.290627479553223\n",
      "Step1 : loss = 8.091072082519531\n",
      "Step2 : loss = 7.896842002868652\n",
      "Step3 : loss = 7.714590072631836\n",
      "Step4 : loss = 7.546658515930176\n",
      "Step5 : loss = 7.3929595947265625\n",
      "Step6 : loss = 7.266097545623779\n",
      "Step7 : loss = 7.169705390930176\n",
      "Step8 : loss = 7.101988315582275\n",
      "Step9 : loss = 7.0578227043151855\n",
      "Data stream Batch- 0 : loss = 7.3312647342681885\n",
      "Meta Update\n",
      "Training is starting\n",
      "Step 0 : loss = 0.11076637073594046\n",
      "Step 1 : loss = 0.11061643812580864\n",
      "Step 2 : loss = 0.11046935284533903\n",
      "Step 3 : loss = 0.11032225057620014\n",
      "Step 4 : loss = 0.11017516561546635\n",
      "Step 5 : loss = 0.11002802106205141\n",
      "Step 6 : loss = 0.10988083393370651\n",
      "Step 7 : loss = 0.10973361310027611\n",
      "Step 8 : loss = 0.1095863582240596\n",
      "Step 9 : loss = 0.10943944431374263\n",
      "Update Procedure\n",
      "Step0 : loss = 5.76216459274292\n",
      "Step1 : loss = 5.348668098449707\n",
      "Step2 : loss = 4.950592756271362\n",
      "Step3 : loss = 4.589355230331421\n",
      "Step4 : loss = 4.277109146118164\n",
      "Step5 : loss = 4.022527575492859\n",
      "Step6 : loss = 3.825447678565979\n",
      "Step7 : loss = 3.6813642978668213\n",
      "Step8 : loss = 3.5837020874023438\n",
      "Step9 : loss = 3.520240545272827\n",
      "Data stream Batch- 1 : loss = 7.092199325561523\n",
      "Meta Update\n",
      "Training is starting\n",
      "Step 0 : loss = 0.10017453446908946\n",
      "Step 1 : loss = 0.1000641415771751\n",
      "Step 2 : loss = 0.09995372315182328\n",
      "Step 3 : loss = 0.09984327885359916\n",
      "Step 4 : loss = 0.09973280966526983\n",
      "Step 5 : loss = 0.09962232311904824\n",
      "Step 6 : loss = 0.0995118192396567\n",
      "Step 7 : loss = 0.09940129043255007\n",
      "Step 8 : loss = 0.09929073578971703\n",
      "Step 9 : loss = 0.09918015557752398\n",
      "Update Procedure\n",
      "Step0 : loss = 3.147296984990438\n",
      "Step1 : loss = 2.672108809153239\n",
      "Step2 : loss = 2.238609870274862\n",
      "Step3 : loss = 1.8862416346867878\n",
      "Step4 : loss = 1.653744061787923\n",
      "Step5 : loss = 1.5114686091740925\n",
      "Step6 : loss = 1.4372611840565999\n",
      "Step7 : loss = 1.3938539028167725\n",
      "Step8 : loss = 1.371624271074931\n",
      "Step9 : loss = 1.3590727647145588\n",
      "Data stream Batch- 2 : loss = 6.895859718322754\n",
      "Meta Update\n",
      "Training is starting\n",
      "Step 0 : loss = 0.09685284174599873\n",
      "Step 1 : loss = 0.0967684299224169\n",
      "Step 2 : loss = 0.09668402666252449\n",
      "Step 3 : loss = 0.09659958906335996\n",
      "Step 4 : loss = 0.09651514289996808\n",
      "Step 5 : loss = 0.09643068007007922\n",
      "Step 6 : loss = 0.09634635313947616\n",
      "Step 7 : loss = 0.09626392023181507\n",
      "Step 8 : loss = 0.09618148270279361\n",
      "Step 9 : loss = 0.09609901897315833\n",
      "Update Procedure\n",
      "Step0 : loss = 1.3425259292125702\n",
      "Step1 : loss = 1.2649462223052979\n",
      "Step2 : loss = 1.1999163329601288\n",
      "Step3 : loss = 1.147514134645462\n",
      "Step4 : loss = 1.0965512692928314\n",
      "Step5 : loss = 1.061523675918579\n",
      "Step6 : loss = 1.0311851799488068\n",
      "Step7 : loss = 1.0100013613700867\n",
      "Step8 : loss = 0.9979769587516785\n",
      "Step9 : loss = 0.990275576710701\n",
      "Data stream Batch- 3 : loss = 6.899116039276123\n",
      "Meta Update\n",
      "Training is starting\n",
      "Step 0 : loss = 0.10513268796211353\n",
      "Step 1 : loss = 0.10505530169285358\n",
      "Step 2 : loss = 0.10497789876921294\n",
      "Step 3 : loss = 0.10490048665032704\n",
      "Step 4 : loss = 0.10482303229486563\n",
      "Step 5 : loss = 0.10474556943651155\n",
      "Step 6 : loss = 0.1046680977225781\n",
      "Step 7 : loss = 0.10459060080193686\n",
      "Step 8 : loss = 0.10451343596354612\n",
      "Step 9 : loss = 0.10443684268971677\n",
      "Update Procedure\n",
      "Step0 : loss = 1.0430377125740051\n",
      "Step1 : loss = 0.9961200952529907\n",
      "Step2 : loss = 0.9737756371498107\n",
      "Step3 : loss = 0.9391189098358155\n",
      "Step4 : loss = 0.9050808191299439\n",
      "Step5 : loss = 0.8873100996017456\n",
      "Step6 : loss = 0.8626812815666198\n",
      "Step7 : loss = 0.8493604302406311\n",
      "Step8 : loss = 0.8374104142189026\n",
      "Step9 : loss = 0.8311755776405334\n",
      "Data stream Batch- 4 : loss = 7.117736339569092\n",
      "Meta Update\n",
      "Training is starting\n",
      "Step 0 : loss = 0.11547505504592312\n",
      "Step 1 : loss = 0.11537471623868736\n",
      "Step 2 : loss = 0.11527431692819011\n",
      "Step 3 : loss = 0.11517391885414739\n",
      "Step 4 : loss = 0.11507416482563018\n",
      "Step 5 : loss = 0.11497442190680733\n",
      "Step 6 : loss = 0.11487471304750721\n",
      "Step 7 : loss = 0.11477558171296003\n",
      "Step 8 : loss = 0.1146764330076707\n",
      "Step 9 : loss = 0.11457725026678303\n",
      "Update Procedure\n",
      "Step0 : loss = 1.955087383588155\n",
      "Step1 : loss = 1.958369066317876\n",
      "Step2 : loss = 1.928827742735545\n",
      "Step3 : loss = 1.9102654258410137\n",
      "Step4 : loss = 1.8643422921498616\n",
      "Step5 : loss = 1.856602321068446\n",
      "Step6 : loss = 1.8339629570643108\n",
      "Step7 : loss = 1.8139971941709518\n",
      "Step8 : loss = 1.805285096168518\n",
      "Step9 : loss = 1.7974515507618587\n",
      "Data stream Batch- 5 : loss = 7.06162166595459\n",
      "Meta Update\n",
      "Training is starting\n",
      "Step 0 : loss = 0.12331737568449631\n",
      "Step 1 : loss = 0.12315732471657802\n",
      "Step 2 : loss = 0.12299749671181372\n",
      "Step 3 : loss = 0.12283759177990072\n",
      "Step 4 : loss = 0.12267764364249166\n",
      "Step 5 : loss = 0.12251768761960506\n",
      "Step 6 : loss = 0.1223591564547374\n",
      "Step 7 : loss = 0.12220056568371705\n",
      "Step 8 : loss = 0.12204194114421954\n",
      "Step 9 : loss = 0.12188323936220316\n",
      "Update Procedure\n",
      "Step0 : loss = 2.618774890899658\n",
      "Step1 : loss = 2.6179688019411906\n",
      "Step2 : loss = 2.5506524784224376\n",
      "Step3 : loss = 2.500448995402881\n",
      "Step4 : loss = 2.497136094740459\n",
      "Step5 : loss = 2.44754989019462\n",
      "Step6 : loss = 2.4239071990762437\n",
      "Step7 : loss = 2.412948340177536\n",
      "Step8 : loss = 2.3953273072838783\n",
      "Step9 : loss = 2.386398494243622\n",
      "Data stream Batch- 6 : loss = 6.55339789390564\n",
      "Meta Update\n",
      "Training is starting\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 : loss = 0.12014892000183768\n",
      "Step 1 : loss = 0.11999513899166768\n",
      "Step 2 : loss = 0.11984129907912125\n",
      "Step 3 : loss = 0.11968742508178003\n",
      "Step 4 : loss = 0.1195334993877279\n",
      "Step 5 : loss = 0.11937953109391052\n",
      "Step 6 : loss = 0.1192255199092295\n",
      "Step 7 : loss = 0.11907144977438336\n",
      "Step 8 : loss = 0.11891735341418742\n",
      "Step 9 : loss = 0.11876319749710686\n",
      "Update Procedure\n",
      "Step0 : loss = 2.8893420416861773\n",
      "Step1 : loss = 2.8541505336761475\n",
      "Step2 : loss = 2.776033755391836\n",
      "Step3 : loss = 2.6824515387415886\n",
      "Step4 : loss = 2.6393204424530268\n",
      "Step5 : loss = 2.602898921817541\n",
      "Step6 : loss = 2.542087503708899\n",
      "Step7 : loss = 2.507130479440093\n",
      "Step8 : loss = 2.480269853025675\n",
      "Step9 : loss = 2.466082966886461\n",
      "Data stream Batch- 7 : loss = 5.463283538818359\n",
      "Task  8\n",
      "Meta Update\n",
      "Training is starting\n",
      "Step 0 : loss = 0.1761104993695812\n",
      "Step 1 : loss = 0.17595496422620646\n",
      "Step 2 : loss = 0.17579938234429193\n",
      "Step 3 : loss = 0.17564369317387385\n",
      "Step 4 : loss = 0.17548795635143966\n",
      "Step 5 : loss = 0.1753321276258117\n",
      "Step 6 : loss = 0.17517622144868905\n",
      "Step 7 : loss = 0.17502025172497615\n",
      "Step 8 : loss = 0.17486419103993914\n",
      "Step 9 : loss = 0.17470805243612406\n",
      "Update Procedure\n",
      "Step0 : loss = 3.6080472469329834\n",
      "Step1 : loss = 3.5055596828460693\n",
      "Step2 : loss = 3.4067394733428955\n",
      "Step3 : loss = 3.314035654067993\n",
      "Step4 : loss = 3.237166404724121\n",
      "Step5 : loss = 3.174896478652954\n",
      "Step6 : loss = 3.1254770755767822\n",
      "Step7 : loss = 3.088937997817993\n",
      "Step8 : loss = 3.0637290477752686\n",
      "Step9 : loss = 3.0473203659057617\n",
      "Data stream Batch- 0 : loss = 5.016483545303345\n",
      "Meta Update\n",
      "Training is starting\n",
      "Step 0 : loss = 0.17343304262278686\n",
      "Step 1 : loss = 0.17327735119045307\n",
      "Step 2 : loss = 0.17312079416104822\n",
      "Step 3 : loss = 0.17296417404958622\n",
      "Step 4 : loss = 0.17280746249626985\n",
      "Step 5 : loss = 0.17265070330716897\n",
      "Step 6 : loss = 0.1724938363327852\n",
      "Step 7 : loss = 0.17233689286616735\n",
      "Step 8 : loss = 0.1721798560558909\n",
      "Step 9 : loss = 0.1720227263784947\n",
      "Update Procedure\n",
      "Step0 : loss = 3.6600507497787476\n",
      "Step1 : loss = 3.5192514657974243\n",
      "Step2 : loss = 3.3894397020339966\n",
      "Step3 : loss = 3.2769140005111694\n",
      "Step4 : loss = 3.1850258111953735\n",
      "Step5 : loss = 3.111929416656494\n",
      "Step6 : loss = 3.056208372116089\n",
      "Step7 : loss = 3.0158568620681763\n",
      "Step8 : loss = 2.9884363412857056\n",
      "Step9 : loss = 2.97066867351532\n",
      "Data stream Batch- 1 : loss = 4.462674140930176\n",
      "Meta Update\n",
      "Training is starting\n",
      "Step 0 : loss = 0.17117091893014785\n",
      "Step 1 : loss = 0.17101156719085833\n",
      "Step 2 : loss = 0.17085215382777072\n",
      "Step 3 : loss = 0.1706926327115343\n",
      "Step 4 : loss = 0.17053306344678174\n",
      "Step 5 : loss = 0.17037337343442463\n",
      "Step 6 : loss = 0.1702144536999418\n",
      "Step 7 : loss = 0.1700546389983801\n",
      "Step 8 : loss = 0.1698947309479801\n",
      "Step 9 : loss = 0.16973476132213902\n",
      "Update Procedure\n",
      "Step0 : loss = 3.2385194301605225\n",
      "Step1 : loss = 3.0711177984873452\n",
      "Step2 : loss = 2.929519255956014\n",
      "Step3 : loss = 2.793778379758199\n",
      "Step4 : loss = 2.68403955300649\n",
      "Step5 : loss = 2.59567928314209\n",
      "Step6 : loss = 2.5322024822235107\n",
      "Step7 : loss = 2.4884023666381836\n",
      "Step8 : loss = 2.457814892133077\n",
      "Step9 : loss = 2.437513510386149\n",
      "Data stream Batch- 2 : loss = 3.747428059577942\n",
      "Meta Update\n",
      "Training is starting\n",
      "Step 0 : loss = 0.16746042639359807\n",
      "Step 1 : loss = 0.16731159250267127\n",
      "Step 2 : loss = 0.16714614073026385\n",
      "Step 3 : loss = 0.1669813082209882\n",
      "Step 4 : loss = 0.16681567265101233\n",
      "Step 5 : loss = 0.16665070202490312\n",
      "Step 6 : loss = 0.16648493939428122\n",
      "Step 7 : loss = 0.16631905644822767\n",
      "Step 8 : loss = 0.16615313934152431\n",
      "Step 9 : loss = 0.16598707017874254\n",
      "Update Procedure\n",
      "Step0 : loss = 2.592716008424759\n",
      "Step1 : loss = 2.4550348818302155\n",
      "Step2 : loss = 2.3122618198394775\n",
      "Step3 : loss = 2.2052656412124634\n",
      "Step4 : loss = 2.1050818860530853\n",
      "Step5 : loss = 2.019665628671646\n",
      "Step6 : loss = 1.9519794881343842\n",
      "Step7 : loss = 1.9032881557941437\n",
      "Step8 : loss = 1.870047003030777\n",
      "Step9 : loss = 1.8488898575305939\n",
      "Data stream Batch- 3 : loss = 3.124548316001892\n",
      "Meta Update\n",
      "Training is starting\n",
      "Step 0 : loss = 0.16848833594461057\n",
      "Step 1 : loss = 0.16831995292908103\n",
      "Step 2 : loss = 0.16815147704539282\n",
      "Step 3 : loss = 0.16798289387279147\n",
      "Step 4 : loss = 0.167814247149232\n",
      "Step 5 : loss = 0.16764549363431294\n",
      "Step 6 : loss = 0.1674766621200747\n",
      "Step 7 : loss = 0.16730773777000013\n",
      "Step 8 : loss = 0.1671387512999548\n",
      "Step 9 : loss = 0.1669696570600675\n",
      "Update Procedure\n",
      "Step0 : loss = 1.999646282196045\n",
      "Step1 : loss = 1.875983428955078\n",
      "Step2 : loss = 1.7634056091308594\n",
      "Step3 : loss = 1.703336763381958\n",
      "Step4 : loss = 1.63243510723114\n",
      "Step5 : loss = 1.5947387218475342\n",
      "Step6 : loss = 1.5496016979217528\n",
      "Step7 : loss = 1.5234036803245545\n",
      "Step8 : loss = 1.5041709303855897\n",
      "Step9 : loss = 1.4924918293952942\n",
      "Data stream Batch- 4 : loss = 2.6742966175079346\n",
      "Meta Update\n",
      "Training is starting\n",
      "Step 0 : loss = 0.1713742453999642\n",
      "Step 1 : loss = 0.17120370912507224\n",
      "Step 2 : loss = 0.17103304971560213\n",
      "Step 3 : loss = 0.17086232865083162\n",
      "Step 4 : loss = 0.17069149842226833\n",
      "Step 5 : loss = 0.17052060604199487\n",
      "Step 6 : loss = 0.17034962130629128\n",
      "Step 7 : loss = 0.17017854367039376\n",
      "Step 8 : loss = 0.17000738854843864\n",
      "Step 9 : loss = 0.169836110323664\n",
      "Update Procedure\n",
      "Step0 : loss = 2.5014413595199585\n",
      "Step1 : loss = 2.4800239900747933\n",
      "Step2 : loss = 2.4380969405174255\n",
      "Step3 : loss = 2.432314465443293\n",
      "Step4 : loss = 2.3794976274172464\n",
      "Step5 : loss = 2.352276861667633\n",
      "Step6 : loss = 2.322925478219986\n",
      "Step7 : loss = 2.3089961608250937\n",
      "Step8 : loss = 2.292865733305613\n",
      "Step9 : loss = 2.28503625591596\n",
      "Data stream Batch- 5 : loss = 2.519975960254669\n",
      "Meta Update\n",
      "Training is starting\n",
      "Step 0 : loss = 0.16753442193359191\n",
      "Step 1 : loss = 0.16735779256717045\n",
      "Step 2 : loss = 0.1671810554808582\n",
      "Step 3 : loss = 0.16700423988167168\n",
      "Step 4 : loss = 0.16682733243841025\n",
      "Step 5 : loss = 0.16665033064960225\n",
      "Step 6 : loss = 0.16647325231979299\n",
      "Step 7 : loss = 0.1662960662212058\n",
      "Step 8 : loss = 0.16611878727048418\n",
      "Step 9 : loss = 0.1659414159165415\n",
      "Update Procedure\n",
      "Step0 : loss = 2.5888993058885847\n",
      "Step1 : loss = 2.6158390513488223\n",
      "Step2 : loss = 2.5325690507888794\n",
      "Step3 : loss = 2.541644662618637\n",
      "Step4 : loss = 2.4853825867176056\n",
      "Step5 : loss = 2.4820217745644704\n",
      "Step6 : loss = 2.4420320221355984\n",
      "Step7 : loss = 2.440128526517323\n",
      "Step8 : loss = 2.4203460769993916\n",
      "Step9 : loss = 2.413811517613275\n",
      "Data stream Batch- 6 : loss = 2.4595115780830383\n",
      "Meta Update\n",
      "Training is starting\n",
      "Step 0 : loss = 0.15347900423955682\n",
      "Step 1 : loss = 0.15328859573862078\n",
      "Step 2 : loss = 0.1530980799498573\n",
      "Step 3 : loss = 0.15290747081227193\n",
      "Step 4 : loss = 0.1527167687758443\n",
      "Step 5 : loss = 0.15252597436712545\n",
      "Step 6 : loss = 0.15233508659465717\n",
      "Step 7 : loss = 0.15214409055534545\n",
      "Step 8 : loss = 0.151952987211917\n",
      "Step 9 : loss = 0.1517618049246463\n",
      "Update Procedure\n",
      "Step0 : loss = 2.88846343010664\n",
      "Step1 : loss = 2.8484446965157986\n",
      "Step2 : loss = 2.7959557846188545\n",
      "Step3 : loss = 2.7626781053841114\n",
      "Step4 : loss = 2.714612551033497\n",
      "Step5 : loss = 2.695367857813835\n",
      "Step6 : loss = 2.6632973477244377\n",
      "Step7 : loss = 2.6450834572315216\n",
      "Step8 : loss = 2.630520759150386\n",
      "Step9 : loss = 2.6219812128692865\n",
      "Data stream Batch- 7 : loss = 2.4512581825256348\n",
      "Task  9\n",
      "Meta Update\n",
      "Training is starting\n",
      "Step 0 : loss = 0.16541111862793884\n",
      "Step 1 : loss = 0.16534689711293912\n",
      "Step 2 : loss = 0.16528270173279186\n",
      "Step 3 : loss = 0.16521851997639206\n",
      "Step 4 : loss = 0.16515432459621884\n",
      "Step 5 : loss = 0.16509014246145648\n",
      "Step 6 : loss = 0.16502600044252802\n",
      "Step 7 : loss = 0.16496183155525573\n",
      "Step 8 : loss = 0.1648976895381079\n",
      "Step 9 : loss = 0.1648335471421966\n",
      "Update Procedure\n",
      "Step0 : loss = 0.6007897853851318\n",
      "Step1 : loss = 0.5971747040748596\n",
      "Step2 : loss = 0.5783449411392212\n",
      "Step3 : loss = 0.5699775815010071\n",
      "Step4 : loss = 0.5537581443786621\n",
      "Step5 : loss = 0.5454686880111694\n",
      "Step6 : loss = 0.533511221408844\n",
      "Step7 : loss = 0.5268543362617493\n",
      "Step8 : loss = 0.5206423997879028\n",
      "Step9 : loss = 0.5170350074768066\n",
      "Data stream Batch- 0 : loss = 2.6554830074310303\n",
      "Meta Update\n",
      "Training is starting\n",
      "Step 0 : loss = 0.160622597058077\n",
      "Step 1 : loss = 0.1605656642580044\n",
      "Step 2 : loss = 0.16050874471421736\n",
      "Step 3 : loss = 0.16045182553758486\n",
      "Step 4 : loss = 0.16039490559276942\n",
      "Step 5 : loss = 0.1603380129182121\n",
      "Step 6 : loss = 0.1602811065969783\n",
      "Step 7 : loss = 0.1602242926158025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 8 : loss = 0.1601684765799688\n",
      "Step 9 : loss = 0.16011266052153228\n",
      "Update Procedure\n",
      "Step0 : loss = 0.6793792247772217\n",
      "Step1 : loss = 0.6781522035598755\n",
      "Step2 : loss = 0.6243791580200195\n",
      "Step3 : loss = 0.6301775574684143\n",
      "Step4 : loss = 0.6029043793678284\n",
      "Step5 : loss = 0.596164733171463\n",
      "Step6 : loss = 0.5846839100122452\n",
      "Step7 : loss = 0.5759836137294769\n",
      "Step8 : loss = 0.5698298811912537\n",
      "Step9 : loss = 0.5660708695650101\n",
      "Data stream Batch- 1 : loss = 2.566380560398102\n",
      "Meta Update\n",
      "Training is starting\n",
      "Step 0 : loss = 0.16188980744697645\n",
      "Step 1 : loss = 0.1618354853494014\n",
      "Step 2 : loss = 0.16178115001750673\n",
      "Step 3 : loss = 0.1617269868994087\n",
      "Step 4 : loss = 0.16167476366419406\n",
      "Step 5 : loss = 0.16162252717236703\n",
      "Step 6 : loss = 0.16157033081749939\n",
      "Step 7 : loss = 0.16151812121714973\n",
      "Step 8 : loss = 0.16146576403513968\n",
      "Step 9 : loss = 0.1614134197316365\n",
      "Update Procedure\n",
      "Step0 : loss = 0.5922774275143942\n",
      "Step1 : loss = 0.587909976641337\n",
      "Step2 : loss = 0.5149739980697632\n",
      "Step3 : loss = 0.5224795043468475\n",
      "Step4 : loss = 0.4842684368292491\n",
      "Step5 : loss = 0.49271177252133685\n",
      "Step6 : loss = 0.4735864500204722\n",
      "Step7 : loss = 0.47141260902086896\n",
      "Step8 : loss = 0.4647289713223775\n",
      "Step9 : loss = 0.460205614566803\n",
      "Data stream Batch- 2 : loss = 2.4527981281280518\n",
      "Meta Update\n",
      "Training is starting\n",
      "Step 0 : loss = 0.16465150233346335\n",
      "Step 1 : loss = 0.16459618782010718\n",
      "Step 2 : loss = 0.1645412544056014\n",
      "Step 3 : loss = 0.1644863599817667\n",
      "Step 4 : loss = 0.16443145267961146\n",
      "Step 5 : loss = 0.16437655864502287\n",
      "Step 6 : loss = 0.1643216641984448\n",
      "Step 7 : loss = 0.16426678453336882\n",
      "Step 8 : loss = 0.16421190411140893\n",
      "Step 9 : loss = 0.1641570372906036\n",
      "Update Procedure\n",
      "Step0 : loss = 0.5986413583159447\n",
      "Step1 : loss = 0.6187241151928902\n",
      "Step2 : loss = 0.5784093737602234\n",
      "Step3 : loss = 0.592676006257534\n",
      "Step4 : loss = 0.5647424161434174\n",
      "Step5 : loss = 0.5665389597415924\n",
      "Step6 : loss = 0.5546151176095009\n",
      "Step7 : loss = 0.5493780821561813\n",
      "Step8 : loss = 0.5446333065629005\n",
      "Step9 : loss = 0.5419473499059677\n",
      "Data stream Batch- 3 : loss = 2.4915144443511963\n",
      "Meta Update\n",
      "Training is starting\n",
      "Step 0 : loss = 0.16578500215642303\n",
      "Step 1 : loss = 0.16572770821463725\n",
      "Step 2 : loss = 0.16567063905761967\n",
      "Step 3 : loss = 0.16561438234428502\n",
      "Step 4 : loss = 0.165558138876434\n",
      "Step 5 : loss = 0.1655019082865928\n",
      "Step 6 : loss = 0.16544567768529075\n",
      "Step 7 : loss = 0.1653894610866897\n",
      "Step 8 : loss = 0.16533325697667556\n",
      "Step 9 : loss = 0.16527705325628067\n",
      "Update Procedure\n",
      "Step0 : loss = 1.1580630838871002\n",
      "Step1 : loss = 1.238678330183029\n",
      "Step2 : loss = 1.1835466146469116\n",
      "Step3 : loss = 1.1834703385829926\n",
      "Step4 : loss = 1.1736335575580596\n",
      "Step5 : loss = 1.1653424739837646\n",
      "Step6 : loss = 1.1515350997447968\n",
      "Step7 : loss = 1.1528996407985688\n",
      "Step8 : loss = 1.1405960083007813\n",
      "Step9 : loss = 1.1382446140050888\n",
      "Data stream Batch- 4 : loss = 2.5707712173461914\n",
      "Meta Update\n",
      "Training is starting\n",
      "Step 0 : loss = 0.16656425284303925\n",
      "Step 1 : loss = 0.16650542266840454\n",
      "Step 2 : loss = 0.16644742987207684\n",
      "Step 3 : loss = 0.1663897167488253\n",
      "Step 4 : loss = 0.1663320437291509\n",
      "Step 5 : loss = 0.16627434381802156\n",
      "Step 6 : loss = 0.1662166571857173\n",
      "Step 7 : loss = 0.16615893079454808\n",
      "Step 8 : loss = 0.16610123089468648\n",
      "Step 9 : loss = 0.16604355748615365\n",
      "Update Procedure\n",
      "Step0 : loss = 1.9927840158343315\n",
      "Step1 : loss = 2.0510526498158774\n",
      "Step2 : loss = 1.9883345390359561\n",
      "Step3 : loss = 1.992460126678149\n",
      "Step4 : loss = 1.971089447538058\n",
      "Step5 : loss = 1.951518254975478\n",
      "Step6 : loss = 1.9422862107555072\n",
      "Step7 : loss = 1.9263871038953464\n",
      "Step8 : loss = 1.9216638108094533\n",
      "Step9 : loss = 1.9160127912958462\n",
      "Data stream Batch- 5 : loss = 2.6394107341766357\n",
      "Meta Update\n",
      "Training is starting\n",
      "Step 0 : loss = 0.1684951356538384\n",
      "Step 1 : loss = 0.1684361751662233\n",
      "Step 2 : loss = 0.1683772021903349\n",
      "Step 3 : loss = 0.16831824207049906\n",
      "Step 4 : loss = 0.16825926871581137\n",
      "Step 5 : loss = 0.16820032147397337\n",
      "Step 6 : loss = 0.1681413882122337\n",
      "Step 7 : loss = 0.1680824810849294\n",
      "Step 8 : loss = 0.16802356034466515\n",
      "Step 9 : loss = 0.16796463959361285\n",
      "Update Procedure\n",
      "Step0 : loss = 2.3840455604451045\n",
      "Step1 : loss = 2.439557132976396\n",
      "Step2 : loss = 2.344773143529892\n",
      "Step3 : loss = 2.3555483732904707\n",
      "Step4 : loss = 2.3160353132656644\n",
      "Step5 : loss = 2.305969672543662\n",
      "Step6 : loss = 2.296505834375109\n",
      "Step7 : loss = 2.2797638263021196\n",
      "Step8 : loss = 2.270432304058756\n",
      "Step9 : loss = 2.2653838204486028\n",
      "Data stream Batch- 6 : loss = 2.652734875679016\n",
      "Meta Update\n",
      "Training is starting\n",
      "Step 0 : loss = 0.16949002799054783\n",
      "Step 1 : loss = 0.16943083088270633\n",
      "Step 2 : loss = 0.16937164778751493\n",
      "Step 3 : loss = 0.1693124775715232\n",
      "Step 4 : loss = 0.16925332017796846\n",
      "Step 5 : loss = 0.16919414958312665\n",
      "Step 6 : loss = 0.16913504518245243\n",
      "Step 7 : loss = 0.1690760641339811\n",
      "Step 8 : loss = 0.16901705619385174\n",
      "Step 9 : loss = 0.16895808797868694\n",
      "Update Procedure\n",
      "Step0 : loss = 2.7768353521823883\n",
      "Step1 : loss = 2.7673753388226032\n",
      "Step2 : loss = 2.705537524074316\n",
      "Step3 : loss = 2.6753964982926846\n",
      "Step4 : loss = 2.6389799285680056\n",
      "Step5 : loss = 2.608739363029599\n",
      "Step6 : loss = 2.5955558884888887\n",
      "Step7 : loss = 2.5768297873437405\n",
      "Step8 : loss = 2.5647792164236307\n",
      "Step9 : loss = 2.5575273241847754\n",
      "Data stream Batch- 7 : loss = 2.6241913437843323\n"
     ]
    }
   ],
   "source": [
    "\n",
    "meta_step = 10\n",
    "loss_ftml1 = []\n",
    "total = []\n",
    "all_eval_loss1 = []\n",
    "all_train_loss1 = []\n",
    "xtask_buffer = []\n",
    "ttask_buffer = []\n",
    "ftml_eval1 = []\n",
    "ftml_time1 = []\n",
    "buffer_length = len(xtask_buffer)\n",
    "for i in range (10):\n",
    "    print(\"Task \", i)\n",
    "    eval_task = []\n",
    "    train_loss = []\n",
    "    start = time.time()\n",
    "\n",
    "    buffer_length = len(xtask_buffer)\n",
    "    for j in range (8):\n",
    "        \n",
    "        total_loss_task = 0\n",
    "        dtstream_x = traintaskx[i][0:j+1]\n",
    "        dtstream_t = traintaskt[i][0:j+1]\n",
    "        \n",
    "        if len(xtask_buffer) <   1:\n",
    "            dtrainx = dvalx = dtstream_x\n",
    "            dtraint = dvalt = dtstream_t\n",
    "        elif len(xtask_buffer) == 1:\n",
    "            dtrainx = dvalx = xtask_buffer \n",
    "            dtraint = dvalt = ttask_buffer\n",
    "        else :\n",
    "            dtrainx = xtask_buffer[0:buffer_length//2]\n",
    "            dvalx = xtask_buffer[buffer_length//2:]\n",
    "            dtraint = ttask_buffer[0:buffer_length//2]\n",
    "            dvalt = ttask_buffer[buffer_length//2:]\n",
    "\n",
    "        print(\"Meta Update\")\n",
    "        ftml1, loss = train_maml(ftml1, meta_step, dtrainx, dtraint, dvalx, dvalt)\n",
    "        total_loss_task += sum(loss)/len(loss)\n",
    "        print(\"Update Procedure\")\n",
    "        ftml1, loss = update_procedure(ftml1,dtstream_x, dtstream_t)\n",
    "        total_loss_task = (total_loss_task + sum(loss)/len(loss))/2\n",
    "\n",
    "        tmp_loss = 0\n",
    "        for k in range(len(valtaskx[i])):\n",
    "            _, loss = model_func(ftml1, valtaskx[i][k], valtaskt[i][k])\n",
    "            tmp_loss+=loss\n",
    "\n",
    "        eval_loss = tmp_loss/2\n",
    "        eval_task.append(eval_loss)\n",
    "        train_loss.append(total_loss_task)\n",
    "\n",
    "        print('Data stream Batch- {} : loss = {}'.format(j,eval_loss))\n",
    "        # if eval_loss < threshold or j == 9:\n",
    "        #     print(\"Training Finish\")\n",
    "        #     total.append(j+1)\n",
    "        #     loss_ftml.append(eval_loss)\n",
    "        #     break\n",
    "    curr = time.time() - start\n",
    "    ftml_time1.append(curr)\n",
    "    start = time.time()\n",
    "    xtask_buffer+=dtstream_x\n",
    "    ttask_buffer+=dtstream_t\n",
    "    ftml_eval1.append(eval_loss)\n",
    "    all_train_loss1.append(train_loss)\n",
    "    all_eval_loss1.append(eval_task)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(1)   \n",
    "plt.plot(ftml_eval1)\n",
    "plt.xlabel('Data Size (index*100)')\n",
    "plt.ylabel('MAE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80\n"
     ]
    }
   ],
   "source": [
    "print(len(xtask_buffer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "meta_step = 10\n",
    "loss_ftml2 = []\n",
    "total = []\n",
    "all_eval_loss2 = []\n",
    "all_train_loss2 = []\n",
    "xtask_buffer = []\n",
    "ttask_buffer = []\n",
    "ftml_eval2 = []\n",
    "ftml_time2 = []\n",
    "buffer_length = len(xtask_buffer)\n",
    "for i in range (10):\n",
    "    print(\"Task \", i)\n",
    "    eval_task = []\n",
    "    train_loss = []\n",
    "    start = time.time()\n",
    "    if len(xtask_buffer) >= 24:\n",
    "        xtask_buffer = xtask_buffer[8:]\n",
    "        ttask_buffer = ttask_buffer[8:] \n",
    "\n",
    "\n",
    "    buffer_length = len(xtask_buffer)\n",
    "    for j in range (8):\n",
    "        \n",
    "        total_loss_task = 0\n",
    "        dtstream_x = traintaskx[i][0:j+1]\n",
    "        dtstream_t = traintaskt[i][0:j+1]\n",
    "        \n",
    "        if len(xtask_buffer) <   1:\n",
    "            dtrainx = dvalx = dtstream_x\n",
    "            dtraint = dvalt = dtstream_t\n",
    "        elif len(xtask_buffer) == 1:\n",
    "            dtrainx = dvalx = xtask_buffer \n",
    "            dtraint = dvalt = ttask_buffer\n",
    "        else :\n",
    "            dtrainx = xtask_buffer[0:buffer_length//2]\n",
    "            dvalx = xtask_buffer[buffer_length//2:]\n",
    "            dtraint = ttask_buffer[0:buffer_length//2]\n",
    "            dvalt = ttask_buffer[buffer_length//2:]\n",
    "\n",
    "        print(\"Meta Update\")\n",
    "        ftml2, loss = train_maml(ftml2, meta_step, dtrainx, dtraint, dvalx, dvalt)\n",
    "        total_loss_task += sum(loss)/len(loss)\n",
    "        print(\"Update Procedure\")\n",
    "        ftml2, loss = update_procedure(ftml2,dtstream_x, dtstream_t)\n",
    "        total_loss_task = (total_loss_task + sum(loss)/len(loss))/2\n",
    "\n",
    "        tmp_loss = 0\n",
    "        for k in range(len(valtaskx[i])):\n",
    "            _, loss = model_func(ftml2, valtaskx[i][k], valtaskt[i][k])\n",
    "            tmp_loss+=loss\n",
    "\n",
    "        eval_loss = tmp_loss/2\n",
    "        eval_task.append(eval_loss)\n",
    "        train_loss.append(total_loss_task)\n",
    "\n",
    "        print('Data stream Batch- {} : loss = {}'.format(j,eval_loss))\n",
    "        # if eval_loss < threshold or j == 9:\n",
    "        #     print(\"Training Finish\")\n",
    "        #     total.append(j+1)\n",
    "        #     loss_ftml.append(eval_loss)\n",
    "        #     break\n",
    "    curr = time.time() - start\n",
    "    ftml_time2.append(curr)\n",
    "    start = time.time()\n",
    "    xtask_buffer+=dtstream_x\n",
    "    ttask_buffer+=dtstream_t\n",
    "    ftml_eval2.append(eval_loss)\n",
    "    all_train_loss2.append(train_loss)\n",
    "    all_eval_loss2.append(eval_task)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1)   \n",
    "plt.plot(ftml_eval2)\n",
    "plt.xlabel('Data Size (index*100)')\n",
    "plt.ylabel('MAE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(xtask_buffer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task  0\n",
      "Meta Update\n",
      "Training is starting\n",
      "0.001\n",
      "Step 0 : loss = 6.6222052574157715\n",
      "0.0009757729755661011\n",
      "Step 1 : loss = 6.619549751281738\n",
      "0.000905463412215599\n",
      "Step 2 : loss = 6.6169538497924805\n",
      "0.0007959536998847742\n",
      "Step 3 : loss = 6.6145405769348145\n",
      "0.000657963412215599\n",
      "Step 4 : loss = 6.6124701499938965\n",
      "0.000505\n",
      "Step 5 : loss = 6.610776901245117\n",
      "0.0003520365877844011\n",
      "Step 6 : loss = 6.6094651222229\n",
      "0.00021404630011522585\n",
      "Step 7 : loss = 6.608546733856201\n",
      "0.00010453658778440107\n",
      "Step 8 : loss = 6.6079888343811035\n",
      "3.4227024433899005e-05\n",
      "Step 9 : loss = 6.607716083526611\n",
      "Update Procedure\n",
      "Step0 : loss = 6.610231876373291\n",
      "Step1 : loss = 6.576554775238037\n",
      "Step2 : loss = 6.5438232421875\n",
      "Step3 : loss = 6.513246536254883\n",
      "Step4 : loss = 6.485702991485596\n",
      "Step5 : loss = 6.462360858917236\n",
      "Step6 : loss = 6.444185256958008\n",
      "Step7 : loss = 6.431084632873535\n",
      "Step8 : loss = 6.42255973815918\n",
      "Step9 : loss = 6.417665958404541\n",
      "Data stream Batch- 0 : loss = 4.729649066925049\n",
      "Meta Update\n",
      "Training is starting\n",
      "0.001\n",
      "Step 0 : loss = 6.853539228439331\n",
      "0.0009757729755661011\n",
      "Step 1 : loss = 6.851207971572876\n",
      "0.000905463412215599\n",
      "Step 2 : loss = 6.8489062786102295\n",
      "0.0007959536998847742\n",
      "Step 3 : loss = 6.8467347621917725\n",
      "0.000657963412215599\n",
      "Step 4 : loss = 6.844802379608154\n",
      "0.000505\n",
      "Step 5 : loss = 6.843181133270264\n",
      "0.0003520365877844011\n",
      "Step 6 : loss = 6.841957092285156\n",
      "0.00021404630011522585\n",
      "Step 7 : loss = 6.841100692749023\n",
      "0.00010453658778440107\n",
      "Step 8 : loss = 6.840576648712158\n",
      "3.4227024433899005e-05\n",
      "Step 9 : loss = 6.840321063995361\n",
      "Update Procedure\n",
      "Step0 : loss = 6.830573797225952\n",
      "Step1 : loss = 6.770992994308472\n",
      "Step2 : loss = 6.715210437774658\n",
      "Step3 : loss = 6.6704981327056885\n",
      "Step4 : loss = 6.633866310119629\n",
      "Step5 : loss = 6.605718612670898\n",
      "Step6 : loss = 6.585058927536011\n",
      "Step7 : loss = 6.571167945861816\n",
      "Step8 : loss = 6.562626838684082\n",
      "Step9 : loss = 6.557861804962158\n",
      "Data stream Batch- 1 : loss = 4.662793874740601\n",
      "Meta Update\n",
      "Training is starting\n",
      "0.001\n",
      "Step 0 : loss = 6.9953469435373945\n",
      "0.0009757729755661011\n",
      "Step 1 : loss = 6.99457319577535\n",
      "0.000905463412215599\n",
      "Step 2 : loss = 6.993815898895264\n",
      "0.0007959536998847742\n",
      "Step 3 : loss = 6.9931126435597735\n",
      "0.000657963412215599\n",
      "Step 4 : loss = 6.9924882253011065\n",
      "0.000505\n",
      "Step 5 : loss = 6.991974751154582\n",
      "0.0003520365877844011\n",
      "Step 6 : loss = 6.991581837336223\n",
      "0.00021404630011522585\n",
      "Step 7 : loss = 6.991307179133098\n",
      "0.00010453658778440107\n",
      "Step 8 : loss = 6.991139809290568\n",
      "3.4227024433899005e-05\n",
      "Step 9 : loss = 6.991057395935059\n",
      "Update Procedure\n",
      "Step0 : loss = 9.166318893432617\n",
      "Step1 : loss = 9.120964845021566\n",
      "Step2 : loss = 9.07333517074585\n",
      "Step3 : loss = 9.026395003000895\n",
      "Step4 : loss = 8.984064420064291\n",
      "Step5 : loss = 8.946952819824219\n",
      "Step6 : loss = 8.917870998382568\n",
      "Step7 : loss = 8.896826585133871\n",
      "Step8 : loss = 8.883265495300293\n",
      "Step9 : loss = 8.875528653462728\n",
      "Data stream Batch- 2 : loss = 4.552397727966309\n",
      "Meta Update\n",
      "Training is starting\n",
      "0.001\n",
      "Step 0 : loss = 3.4243240157763166\n",
      "0.0009757729755661011\n",
      "Step 1 : loss = 3.4236582318941755\n",
      "0.000905463412215599\n",
      "Step 2 : loss = 3.423009693622589\n",
      "0.0007959536998847742\n",
      "Step 3 : loss = 3.4224096337954206\n",
      "0.000657963412215599\n",
      "Step 4 : loss = 3.421881854534149\n",
      "0.000505\n",
      "Step 5 : loss = 3.4214457273483276\n",
      "0.0003520365877844011\n",
      "Step 6 : loss = 3.4211116035779314\n",
      "0.00021404630011522585\n",
      "Step 7 : loss = 3.4208773970603943\n",
      "0.00010453658778440107\n",
      "Step 8 : loss = 3.4207352598508196\n",
      "3.4227024433899005e-05\n",
      "Step 9 : loss = 3.420665979385376\n",
      "Update Procedure\n",
      "Step0 : loss = 8.352441668510437\n",
      "Step1 : loss = 8.262590646743774\n",
      "Step2 : loss = 8.176399946212769\n",
      "Step3 : loss = 8.084525227546692\n",
      "Step4 : loss = 7.998793959617615\n",
      "Step5 : loss = 7.921568036079407\n",
      "Step6 : loss = 7.860488533973694\n",
      "Step7 : loss = 7.815813064575195\n",
      "Step8 : loss = 7.7870800495147705\n",
      "Step9 : loss = 7.7709044218063354\n",
      "Data stream Batch- 3 : loss = 4.370362281799316\n",
      "Meta Update\n",
      "Training is starting\n",
      "0.001\n",
      "Step 0 : loss = 2.5099334359169005\n",
      "0.0009757729755661011\n",
      "Step 1 : loss = 2.5068043549855554\n",
      "0.000905463412215599\n",
      "Step 2 : loss = 2.503753940264384\n",
      "0.0007959536998847742\n",
      "Step 3 : loss = 2.500916882356008\n",
      "0.000657963412215599\n",
      "Step 4 : loss = 2.4984338680903115\n",
      "0.000505\n",
      "Step 5 : loss = 2.4963818987210593\n",
      "0.0003520365877844011\n",
      "Step 6 : loss = 2.4948091308275857\n",
      "0.00021404630011522585\n",
      "Step 7 : loss = 2.493712147076925\n",
      "0.00010453658778440107\n",
      "Step 8 : loss = 2.4930453260739647\n",
      "3.4227024433899005e-05\n",
      "Step 9 : loss = 2.4927207986513773\n",
      "Update Procedure\n",
      "Step0 : loss = 8.051677036285401\n",
      "Step1 : loss = 7.879301071166992\n",
      "Step2 : loss = 7.727015781402588\n",
      "Step3 : loss = 7.56711597442627\n",
      "Step4 : loss = 7.438047313690186\n",
      "Step5 : loss = 7.320537185668945\n",
      "Step6 : loss = 7.232694339752197\n",
      "Step7 : loss = 7.163259887695313\n",
      "Step8 : loss = 7.120971584320069\n",
      "Step9 : loss = 7.096778202056885\n",
      "Data stream Batch- 4 : loss = 4.051230430603027\n",
      "Meta Update\n",
      "Training is starting\n",
      "0.001\n",
      "Step 0 : loss = 1.7752325140767626\n",
      "0.0009757729755661011\n",
      "Step 1 : loss = 1.7748080369498993\n",
      "0.000905463412215599\n",
      "Step 2 : loss = 1.7743983265426424\n",
      "0.0007959536998847742\n",
      "Step 3 : loss = 1.7740159074465434\n",
      "0.000657963412215599\n",
      "Step 4 : loss = 1.7736837698353662\n",
      "0.000505\n",
      "Step 5 : loss = 1.7734044975704617\n",
      "0.0003520365877844011\n",
      "Step 6 : loss = 1.7731905539830526\n",
      "0.00021404630011522585\n",
      "Step 7 : loss = 1.773041330443488\n",
      "0.00010453658778440107\n",
      "Step 8 : loss = 1.7729509029123518\n",
      "3.4227024433899005e-05\n",
      "Step 9 : loss = 1.772906545466847\n",
      "Update Procedure\n",
      "Step0 : loss = 7.273119489351909\n",
      "Step1 : loss = 7.09864095846812\n",
      "Step2 : loss = 6.9346838394800825\n",
      "Step3 : loss = 6.8031301101048784\n",
      "Step4 : loss = 6.692506154378255\n",
      "Step5 : loss = 6.596765557924907\n",
      "Step6 : loss = 6.52401602268219\n",
      "Step7 : loss = 6.467297196388245\n",
      "Step8 : loss = 6.43117634455363\n",
      "Step9 : loss = 6.410096844037374\n",
      "Data stream Batch- 5 : loss = 3.5340770483016968\n",
      "Meta Update\n",
      "Training is starting\n",
      "0.001\n",
      "Step 0 : loss = 0.9942942010504858\n",
      "0.0009757729755661011\n",
      "Step 1 : loss = 0.9940883675264933\n",
      "0.000905463412215599\n",
      "Step 2 : loss = 0.9938879639383346\n",
      "0.0007959536998847742\n",
      "Step 3 : loss = 0.9937021985886589\n",
      "0.000657963412215599\n",
      "Step 4 : loss = 0.9935400095250871\n",
      "0.000505\n",
      "Step 5 : loss = 0.9934031601936099\n",
      "0.0003520365877844011\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-2be9338f240c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Meta Update\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m         \u001b[0mftml3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_maml\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mftml3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeta_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtrainx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtraint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdvalx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdvalt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mca\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m         \u001b[0mtotal_loss_task\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Update Procedure\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Work\\Labwork\\vr_motion_prediction\\research\\maml.py\u001b[0m in \u001b[0;36mtrain_maml\u001b[1;34m(model, epochs, traintaskx, traintaskt, valtaskx, valtaskt, inner_loop, lr_inner, lr_outer_max, lr_outer_min, log_step, ca, da)\u001b[0m\n\u001b[0;32m     70\u001b[0m                     \u001b[1;31m#step 5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m                     \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0minner_tape\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m                         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minner_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m                     \u001b[1;31m#step 6\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m                     \u001b[0mgradients2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minner_tape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minner_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Work\\Labwork\\vr_motion_prediction\\research\\maml.py\u001b[0m in \u001b[0;36mmodel_func\u001b[1;34m(model, x_train, t_train)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmodel_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m#compute loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMeanAbsoluteError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf.2.x\\lib\\site-packages\\tensorflow\\python\\keras\\losses.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, y_true, y_pred, sample_weight)\u001b[0m\n\u001b[0;32m    141\u001b[0m         y_true, y_pred, sample_weight)\n\u001b[0;32m    142\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name_scope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgraph_ctx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 143\u001b[1;33m       \u001b[0mlosses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    144\u001b[0m       return losses_utils.compute_weighted_loss(\n\u001b[0;32m    145\u001b[0m           losses, sample_weight, reduction=self._get_reduction())\n",
      "\u001b[1;32m~\\.conda\\envs\\tf.2.x\\lib\\site-packages\\tensorflow\\python\\keras\\losses.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, y_true, y_pred)\u001b[0m\n\u001b[0;32m    244\u001b[0m       y_pred, y_true = tf_losses_util.squeeze_or_expand_dimensions(\n\u001b[0;32m    245\u001b[0m           y_pred, y_true)\n\u001b[1;32m--> 246\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fn_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    247\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    248\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mget_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf.2.x\\lib\\site-packages\\tensorflow\\python\\keras\\losses.py\u001b[0m in \u001b[0;36mmean_absolute_error\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m   1228\u001b[0m   \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor_v2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1229\u001b[0m   \u001b[0my_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1230\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1231\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1232\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf.2.x\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36mbinary_op_wrapper\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m    982\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    983\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 984\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    985\u001b[0m       \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msparse_tensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSparseTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    986\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf.2.x\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36msub\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m  10089\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0;32m  10090\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Sub\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mop_callbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m> 10091\u001b[1;33m         x, y)\n\u001b[0m\u001b[0;32m  10092\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  10093\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "meta_step = 10\n",
    "loss_ftml3 = []\n",
    "total = []\n",
    "all_eval_loss3 = []\n",
    "all_train_loss3 = []\n",
    "xtask_buffer = []\n",
    "ttask_buffer = []\n",
    "ftml_eval3 = []\n",
    "ftml_time3 = []\n",
    "buffer_length = len(xtask_buffer)\n",
    "for i in range (10):\n",
    "    print(\"Task \", i)\n",
    "    eval_task = []\n",
    "    train_loss = []\n",
    "    start = time.time()\n",
    "\n",
    "    buffer_length = len(xtask_buffer)\n",
    "    for j in range (8):\n",
    "        \n",
    "        total_loss_task = 0\n",
    "        dtstream_x = traintaskx[i][0:j+1]\n",
    "        dtstream_t = traintaskt[i][0:j+1]\n",
    "        \n",
    "        if len(xtask_buffer) <   1:\n",
    "            dtrainx = dvalx = dtstream_x\n",
    "            dtraint = dvalt = dtstream_t\n",
    "        elif len(xtask_buffer) == 1:\n",
    "            dtrainx = dvalx = xtask_buffer \n",
    "            dtraint = dvalt = ttask_buffer\n",
    "        else :\n",
    "            dtrainx = xtask_buffer[0:buffer_length//2]\n",
    "            dvalx = xtask_buffer[buffer_length//2:]\n",
    "            dtraint = ttask_buffer[0:buffer_length//2]\n",
    "            dvalt = ttask_buffer[buffer_length//2:]\n",
    "\n",
    "        print(\"Meta Update\")\n",
    "        ftml3, loss = train_maml(ftml3, meta_step, dtrainx, dtraint, dvalx, dvalt, ca=True)\n",
    "        total_loss_task += sum(loss)/len(loss)\n",
    "        print(\"Update Procedure\")\n",
    "        ftml3, loss = update_procedure(ftml3,dtstream_x, dtstream_t, ca=True)\n",
    "        total_loss_task = (total_loss_task + sum(loss)/len(loss))/2\n",
    "\n",
    "        tmp_loss = 0\n",
    "        for k in range(len(valtaskx[i])):\n",
    "            _, loss = model_func(ftml3, valtaskx[i][k], valtaskt[i][k])\n",
    "            tmp_loss+=loss\n",
    "\n",
    "        eval_loss = tmp_loss/2\n",
    "        eval_task.append(eval_loss)\n",
    "        train_loss.append(total_loss_task)\n",
    "\n",
    "        print('Data stream Batch- {} : loss = {}'.format(j,eval_loss))\n",
    "        # if eval_loss < threshold or j == 9:\n",
    "        #     print(\"Training Finish\")\n",
    "        #     total.append(j+1)\n",
    "        #     loss_ftml.append(eval_loss)\n",
    "        #     break\n",
    "    curr = time.time() - start\n",
    "    ftml_time3.append(curr)\n",
    "    start = time.time()\n",
    "    xtask_buffer+=dtstream_x\n",
    "    ttask_buffer+=dtstream_t\n",
    "    ftml_eval3.append(eval_loss)\n",
    "    all_train_loss3.append(train_loss)\n",
    "    all_eval_loss3.append(eval_task)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80\n"
     ]
    }
   ],
   "source": [
    "print(len(xtask_buffer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(1)   \n",
    "plt.plot(ftml_eval3)\n",
    "plt.xlabel('Data Size (index*100)')\n",
    "plt.ylabel('MAE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "meta_step = 10\n",
    "loss_ftml4 = []\n",
    "total = []\n",
    "all_eval_loss4 = []\n",
    "all_train_loss4 = []\n",
    "xtask_buffer = []\n",
    "ttask_buffer = []\n",
    "ftml_eval4 = []\n",
    "ftml_time4 = []\n",
    "buffer_length = len(xtask_buffer)\n",
    "for i in range (10):\n",
    "    print(\"Task \", i)\n",
    "    eval_task = []\n",
    "    train_loss = []\n",
    "    start = time.time()\n",
    "\n",
    "    buffer_length = len(xtask_buffer)\n",
    "    for j in range (8):\n",
    "        \n",
    "        total_loss_task = 0\n",
    "        dtstream_x = traintaskx[i][0:j+1]\n",
    "        dtstream_t = traintaskt[i][0:j+1]\n",
    "        \n",
    "        if len(xtask_buffer) <   1:\n",
    "            dtrainx = dvalx = dtstream_x\n",
    "            dtraint = dvalt = dtstream_t\n",
    "        elif len(xtask_buffer) == 1:\n",
    "            dtrainx = dvalx = xtask_buffer \n",
    "            dtraint = dvalt = ttask_buffer\n",
    "        else :\n",
    "            dtrainx = xtask_buffer[0:buffer_length//2]\n",
    "            dvalx = xtask_buffer[buffer_length//2:]\n",
    "            dtraint = ttask_buffer[0:buffer_length//2]\n",
    "            dvalt = ttask_buffer[buffer_length//2:]\n",
    "\n",
    "        print(\"Meta Update\")\n",
    "\n",
    "        ftml4, loss = train_maml(ftml4, meta_step, dtrainx, dtraint, dvalx, dvalt, da=True)\n",
    "        total_loss_task += sum(loss)/len(loss)\n",
    "        print(\"Update Procedure\")\n",
    "        ftml, loss = update_procedure(ftml4,dtstream_x, dtstream_t)\n",
    "        total_loss_task = (total_loss_task + sum(loss)/len(loss))/2\n",
    "\n",
    "        tmp_loss = 0\n",
    "        for k in range(len(valtaskx[i])):\n",
    "            _, loss = model_func(ftml4, valtaskx[i][k], valtaskt[i][k])\n",
    "            tmp_loss+=loss\n",
    "\n",
    "        eval_loss = tmp_loss/2\n",
    "        eval_task.append(eval_loss)\n",
    "        train_loss.append(total_loss_task)\n",
    "\n",
    "        print('Data stream Batch- {} : loss = {}'.format(j,eval_loss))\n",
    "        # if eval_loss < threshold or j == 9:\n",
    "        #     print(\"Training Finish\")\n",
    "        #     total.append(j+1)\n",
    "        #     loss_ftml.append(eval_loss)\n",
    "        #     break\n",
    "    curr = time.time() - start\n",
    "    ftml_time4.append(curr)\n",
    "    start = time.time()\n",
    "    xtask_buffer+=dtstream_x\n",
    "    ttask_buffer+=dtstream_t\n",
    "    ftml_eval4.append(eval_loss)\n",
    "    all_train_loss4.append(train_loss)\n",
    "    all_eval_loss4.append(eval_task)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1)   \n",
    "plt.plot(ftml_eval4)\n",
    "plt.xlabel('Data Size (index*100)')\n",
    "plt.ylabel('MAE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "meta_step = 10\n",
    "loss_ftml5 = []\n",
    "total = []\n",
    "all_eval_loss5 = []\n",
    "all_train_loss5 = []\n",
    "xtask_buffer = []\n",
    "ttask_buffer = []\n",
    "ftml_eval5 = []\n",
    "ftml_time5 = []\n",
    "buffer_length = len(xtask_buffer)\n",
    "for i in range (10):\n",
    "    print(\"Task \", i)\n",
    "    eval_task = []\n",
    "    train_loss = []\n",
    "    start = time.time()\n",
    "\n",
    "    buffer_length = len(xtask_buffer)\n",
    "    for j in range (8):\n",
    "        \n",
    "        total_loss_task = 0\n",
    "        dtstream_x = traintaskx[i][0:j+1]\n",
    "        dtstream_t = traintaskt[i][0:j+1]\n",
    "        \n",
    "        if len(xtask_buffer) <   1:\n",
    "            dtrainx = dvalx = dtstream_x\n",
    "            dtraint = dvalt = dtstream_t\n",
    "        elif len(xtask_buffer) == 1:\n",
    "            dtrainx = dvalx = xtask_buffer \n",
    "            dtraint = dvalt = ttask_buffer\n",
    "        else :\n",
    "            dtrainx = xtask_buffer[0:buffer_length//2]\n",
    "            dvalx = xtask_buffer[buffer_length//2:]\n",
    "            dtraint = ttask_buffer[0:buffer_length//2]\n",
    "            dvalt = ttask_buffer[buffer_length//2:]\n",
    "\n",
    "        print(\"Meta Update\")\n",
    "        ftml5, loss = train_maml_msl(ftml5, meta_step, dtrainx, dtraint, dvalx, dvalt)\n",
    "        total_loss_task += sum(loss)/len(loss)\n",
    "        print(\"Update Procedure\")\n",
    "        ftml5, loss = update_procedure(ftml5,dtstream_x, dtstream_t)\n",
    "        total_loss_task = (total_loss_task + sum(loss)/len(loss))/2\n",
    "\n",
    "        tmp_loss = 0\n",
    "        for k in range(len(valtaskx[i])):\n",
    "            _, loss = model_func(ftml5, valtaskx[i][k], valtaskt[i][k])\n",
    "            tmp_loss+=loss\n",
    "\n",
    "        eval_loss = tmp_loss/2\n",
    "        eval_task.append(eval_loss)\n",
    "        train_loss.append(total_loss_task)\n",
    "\n",
    "        print('Data stream Batch- {} : loss = {}'.format(j,eval_loss))\n",
    "        # if eval_loss < threshold or j == 9:\n",
    "        #     print(\"Training Finish\")\n",
    "        #     total.append(j+1)\n",
    "        #     loss_ftml.append(eval_loss)\n",
    "        #     break\n",
    "    curr = time.time() - start\n",
    "    ftml_time5.append(curr)\n",
    "    start = time.time()\n",
    "    xtask_buffer+=dtstream_x\n",
    "    ttask_buffer+=dtstream_t\n",
    "    ftml_eval5.append(eval_loss)\n",
    "    all_train_loss5.append(train_loss)\n",
    "    all_eval_loss5.append(eval_task)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1)   \n",
    "plt.plot(ftml_eval5)\n",
    "plt.xlabel('Data Size (index*100)')\n",
    "plt.ylabel('MAE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "meta_step = 10\n",
    "loss_ftml6 = []\n",
    "total = []\n",
    "all_eval_loss6 = []\n",
    "all_train_loss6 = []\n",
    "xtask_buffer = []\n",
    "ttask_buffer = []\n",
    "ftml_eval6 = []\n",
    "ftml_time6 = []\n",
    "buffer_length = len(xtask_buffer)\n",
    "for i in range (10):\n",
    "    print(\"Task \", i)\n",
    "    eval_task = []\n",
    "    train_loss = []\n",
    "    start = time.time()\n",
    "    if len(xtask_buffer) >= 24:\n",
    "        xtask_buffer = xtask_buffer[8:]\n",
    "        ttask_buffer = ttask_buffer[8:] \n",
    "\n",
    "\n",
    "    buffer_length = len(xtask_buffer)\n",
    "    for j in range (8):\n",
    "        \n",
    "        total_loss_task = 0\n",
    "        dtstream_x = traintaskx[i][0:j+1]\n",
    "        dtstream_t = traintaskt[i][0:j+1]\n",
    "        \n",
    "        if len(xtask_buffer) <   1:\n",
    "            dtrainx = dvalx = dtstream_x\n",
    "            dtraint = dvalt = dtstream_t\n",
    "        elif len(xtask_buffer) == 1:\n",
    "            dtrainx = dvalx = xtask_buffer \n",
    "            dtraint = dvalt = ttask_buffer\n",
    "        else :\n",
    "            dtrainx = xtask_buffer[0:buffer_length//2]\n",
    "            dvalx = xtask_buffer[buffer_length//2:]\n",
    "            dtraint = ttask_buffer[0:buffer_length//2]\n",
    "            dvalt = ttask_buffer[buffer_length//2:]\n",
    "\n",
    "        print(\"Meta Update\")\n",
    "\n",
    "        ftml6, loss = train_maml_msl(ftml6, meta_step, dtrainx, dtraint, dvalx, dvalt, ca=True, da=True)\n",
    "        total_loss_task += sum(loss)/len(loss)\n",
    "        print(\"Update Procedure\")\n",
    "        ftml6, loss = update_procedure(ftml6,dtstream_x, dtstream_t, ca=True)\n",
    "        total_loss_task = (total_loss_task + sum(loss)/len(loss))/2\n",
    "\n",
    "        tmp_loss = 0\n",
    "        for k in range(len(valtaskx[i])):\n",
    "            _, loss = model_func(ftml6, valtaskx[i][k], valtaskt[i][k])\n",
    "            tmp_loss+=loss\n",
    "\n",
    "        eval_loss = tmp_loss/2\n",
    "        eval_task.append(eval_loss)\n",
    "        train_loss.append(total_loss_task)\n",
    "\n",
    "        print('Data stream Batch- {} : loss = {}'.format(j,eval_loss))\n",
    "        # if eval_loss < threshold or j == 9:\n",
    "        #     print(\"Training Finish\")\n",
    "        #     total.append(j+1)\n",
    "        #     loss_ftml.append(eval_loss)\n",
    "        #     break\n",
    "    curr = time.time() - start\n",
    "    ftml_time6.append(curr)\n",
    "    start = time.time()\n",
    "    xtask_buffer+=dtstream_x\n",
    "    ttask_buffer+=dtstream_t\n",
    "    ftml_eval6.append(eval_loss)\n",
    "    all_train_loss6.append(train_loss)\n",
    "    all_eval_loss6.append(eval_task)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1)   \n",
    "plt.plot(ftml_eval6)\n",
    "plt.xlabel('Data Size (index*100)')\n",
    "plt.ylabel('MAE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(1)   \n",
    "plt.plot(ftml_eval1)\n",
    "plt.plot(ftml_eval2)\n",
    "plt.plot(ftml_eval3)\n",
    "plt.plot(ftml_eval4)\n",
    "plt.plot(ftml_eval5)\n",
    "plt.plot(ftml_eval6)\n",
    "plt.legend([\"Normal\", \"Fixed Buffer\", \"CA\", \"DA\", \"MSL\", \"++\"], loc=(1.05, 0.5))\n",
    "plt.xlabel('Task Index')\n",
    "plt.ylabel('MAE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1)   \n",
    "plt.plot(ftml_eval1[5:])\n",
    "plt.plot(ftml_eval2[5:])\n",
    "plt.plot(ftml_eval3[5:])\n",
    "plt.plot(ftml_eval4[5:])\n",
    "plt.plot(ftml_eval5[5:])\n",
    "plt.plot(ftml_eval6[5:])\n",
    "plt.legend([\"Normal\", \"Fixed Buffer\", \"CA\", \"DA\", \"MSL\", \"++\"], loc=(1.05, 0.5))\n",
    "plt.xlabel('Task Index')\n",
    "plt.ylabel('MAE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(1)  \n",
    "# plt.plot(ftml_eval)\n",
    "\n",
    "# plt.plot(jt_res)\n",
    "# plt.plot(toe_res)\n",
    "# plt.plot(sc_res)\n",
    "\n",
    "\n",
    "# plt.legend([\"Scratch\", \"Joint\", \"TOE\", \"FTML\"], loc=(1.05, 0.5))\n",
    "# plt.xlabel('Task Index')\n",
    "# plt.ylabel('MAE')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    plt.title(\"Task \"+ str(i))\n",
    "    plt.plot(all_eval_loss1[i])\n",
    "    plt.plot(all_eval_loss2[i])\n",
    "    plt.plot(all_eval_loss3[i])\n",
    "    plt.plot(all_eval_loss4[i])\n",
    "    plt.plot(all_eval_loss5[i])\n",
    "    plt.plot(all_eval_loss6[i])\n",
    "\n",
    "    plt.legend([\"Normal\", \"Fixed Buffer\", \"CA\", \"DA\", \"MSL\", \"++\"], loc=(1.05, 0.5))\n",
    "    plt.xlabel('Batch index')\n",
    "    plt.ylabel('MAE')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(1)   \n",
    "# plt.plot(sc_time)\n",
    "# plt.plot(jt_time)\n",
    "# plt.plot(toe_time)\n",
    "# plt.plot(ftml_time)\n",
    "\n",
    "# plt.legend([\"Scratch\", \"Joint\", \"TOE\", \"FTML\"], loc=(1.05, 0.5))\n",
    "# plt.xlabel('Task Index')\n",
    "# plt.ylabel('Second')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1)   \n",
    "plt.plot(ftml_time1)\n",
    "plt.plot(ftml_time2)\n",
    "plt.plot(ftml_time3)\n",
    "plt.plot(ftml_time4)\n",
    "plt.plot(ftml_time5)\n",
    "plt.plot(ftml_eval6)\n",
    "plt.legend([\"Normal\", \"Fixed Buffer\", \"CA\", \"DA\", \"MSL\", \"++\"], loc=(1.05, 0.5))\n",
    "plt.xlabel('Task Index')\n",
    "plt.ylabel('Second')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
