{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import os\n",
    "import sys\n",
    "from scipy import signal\n",
    "import skinematics as skin\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from scipy.stats import norm\n",
    "#from scipy import signal\n",
    "from taskcaller import taskcaller\n",
    "from taskcaller_train1 import taskcaller_train1\n",
    "import random\n",
    "# modularized library import\n",
    "from train_test_split_k import train_test_split_k\n",
    "from rms import rms    \n",
    "from copy import copy\n",
    "from datetime import datetime\n",
    "from maml import *\n",
    "\n",
    "from math import pi\n",
    "from math import cos\n",
    "from math import floor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version:  3.6.10 |Anaconda, Inc.| (default, May  7 2020, 19:46:08) [MSC v.1916 64 bit (AMD64)]\n",
      "Tensorflow version:  2.2.0\n"
     ]
    }
   ],
   "source": [
    "dtype=\"float64\"\n",
    "tf.keras.backend.set_floatx(dtype)\n",
    "print('Python version: ', sys.version)\n",
    "print('Tensorflow version: ', tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../dataset/trainingtask1.csv loaded...\n",
      "\n",
      "Anticipation time: 300ms\n",
      "\n",
      "../dataset/trainingtask2.csv loaded...\n",
      "\n",
      "Anticipation time: 300ms\n",
      "\n",
      "../dataset/trainingtask3.csv loaded...\n",
      "\n",
      "Anticipation time: 300ms\n",
      "\n",
      "../dataset/trainingtask4.csv loaded...\n",
      "\n",
      "Anticipation time: 300ms\n",
      "\n",
      "../dataset/trainingtask5.csv loaded...\n",
      "\n",
      "Anticipation time: 300ms\n",
      "\n",
      "../dataset/trainingtask6.csv loaded...\n",
      "\n",
      "Anticipation time: 300ms\n",
      "\n",
      "../dataset/trainingtask7.csv loaded...\n",
      "\n",
      "Anticipation time: 300ms\n",
      "\n",
      "../dataset/trainingtask8.csv loaded...\n",
      "\n",
      "Anticipation time: 300ms\n",
      "\n",
      "../dataset/trainingtask9.csv loaded...\n",
      "\n",
      "Anticipation time: 300ms\n",
      "\n",
      "../dataset/trainingtask9.csv loaded...\n",
      "\n",
      "Anticipation time: 300ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "S=30\n",
    "# np.random.seed(S)\n",
    "# random.seed(S)\n",
    "# tf.random.set_seed(S)\n",
    "\n",
    "system_rate = 60\n",
    "k_train = 100\n",
    "x_train1, t_train1, x_val1, t_val1,input_nm, target_nm, data_length, DELAY_SIZE, train_eule_data, anticipation_size, train_time_data = taskcaller_train1('../dataset/trainingtask1.csv', system_rate, k_train)\n",
    "x_train2, t_train2, x_val2, t_val2,_, _, _, _, _, _, _ = taskcaller_train1('../dataset/trainingtask2.csv', system_rate, k_train)\n",
    "x_train3, t_train3, x_val3, t_val3,_, _, _, _, _, _, _ = taskcaller_train1('../dataset/trainingtask3.csv', system_rate, k_train)\n",
    "x_train4, t_train4, x_val4, t_val4, _, _, _, _, _, _, _ = taskcaller_train1('../dataset/trainingtask4.csv', system_rate, k_train)\n",
    "x_train5, t_train5, x_val5, t_val5,_, _, _, _, _, _, _ = taskcaller_train1('../dataset/trainingtask5.csv', system_rate, k_train)\n",
    "x_train6, t_train6, x_val6, t_val6,_, _, _, _, _, _, _ = taskcaller_train1('../dataset/trainingtask6.csv', system_rate, k_train)\n",
    "x_train7, t_train7, x_val7, t_val7,_, _, _, _, _, _, _ = taskcaller_train1('../dataset/trainingtask7.csv', system_rate, k_train)\n",
    "x_train8, t_train8, x_val8, t_val8,_, _, _, _, _, _, _ = taskcaller_train1('../dataset/trainingtask8.csv', system_rate, k_train)\n",
    "x_train9, t_train9, x_val9, t_val9,_, _, _, _, _, _, _ = taskcaller_train1('../dataset/trainingtask9.csv', system_rate, k_train)\n",
    "x_train10, t_train10, x_val10, t_val10,_, _, _, _, _, _, _ = taskcaller_train1('../dataset/trainingtask9.csv', system_rate, k_train)\n",
    "#x_seq10, t_seq10, _, _,_, _, _, _, _, _, _ = taskcaller('trainingtask10.csv', system_rate, k)\n",
    "\n",
    "\n",
    "traintaskx = [x_train1 , x_train2 , x_train3 , x_train4,x_train5,x_train6,x_train7,x_train8,x_train9,x_train10]\n",
    "traintaskt = [t_train1 , t_train2 , t_train3 , t_train4,t_train5,t_train6,t_train7, t_train8, t_train9,t_train10]\n",
    "\n",
    "valtaskx = [x_val1,x_val2,x_val3,x_val4,x_val5,x_val6,x_val7,x_val8,x_val9,x_val10]\n",
    "valtaskt = [t_val1,t_val2,t_val3,t_val4,t_val5,t_val6,t_val7,t_val8,t_val9,t_val10]\n",
    "\n",
    "numberoftask = len(traintaskx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.InputLayer(input_shape=(DELAY_SIZE, input_nm)),\n",
    "        tf.keras.layers.Conv1D(27, DELAY_SIZE, activation=tf.nn.relu, input_shape=(DELAY_SIZE, input_nm), use_bias=True, \n",
    "        kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(9, activation=tf.nn.relu, use_bias=True, kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(target_nm, activation='linear', use_bias=True, kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_procedure(model,dtx, dty, lr = 0.001, grad_step =10):\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate = lr)\n",
    "    all_loss = []\n",
    "    for step in range (grad_step):\n",
    "        total_loss = 0\n",
    "        for i in range(len(dtx)):\n",
    "            with tf.GradientTape() as update:\n",
    "                _,loss = model_func(model, dtx[i], dty[i])\n",
    "            gradient = update.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradient, model.trainable_variables))\n",
    "            total_loss+=loss\n",
    "        all_loss.append(total_loss/len(dtx))\n",
    "        print('Step{} : loss = {}'.format(step,total_loss/len(dtx)))\n",
    "    return model, all_loss  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
